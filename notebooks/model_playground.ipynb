{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from model.decoderMLP import decoderMLP, decoderAttention, movieTransformer\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "import pickle \n",
    "import argparse\n",
    "from typing import List\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from pprint import pprint as pp\n",
    "from scipy.sparse import csr_matrix\n",
    "import os \n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from helper.sampler import NegSampler, negsamp_vectorized_bsearch_preverif\n",
    "from argparse import ArgumentParser\n",
    "from model.MF import MatrixFactorization,MatrixFactorizationLLM\n",
    "from trainer.training_utils import *\n",
    "from helper.eval_metrics import *\n",
    "from helper.dataloader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr= 0.00001\n",
    "epochs = 400\n",
    "num_heads = 6\n",
    "cosine = False\n",
    "num_layers = 3\n",
    "output_emb = 64\n",
    "embedding_dim = 768\n",
    "saved_path = f'../saved_model_cluster/ml-100k/attn_best_model_{lr}_{epochs}_{num_heads}_{cosine}_{num_layers}.pth'\n",
    "\n",
    "model_path = saved_path + '_best_model.pth'\n",
    "embedder_path = saved_path + '_embedder.pth'\n",
    "item_embeddings_path = saved_path + '_item_embeddings.pth'\n",
    "user_embeddings_path = saved_path + '_user_embeddings.pth'\n",
    "model_rankings_path = saved_path + '_rankings_matrix.npy'\n",
    "id_genre_map = map_id_to_genre('../data/ml-100k/movies.dat')\n",
    "\n",
    "# 1. Data Loading & Preprocessing\n",
    "train_data = load_dataset(\"../data_preprocessed/ml-100k/data_split/train_set_leave_one.json\")\n",
    "valid_data = load_dataset(\"../data_preprocessed/ml-100k/data_split/valid_set_leave_one.json\")\n",
    "test_data = load_dataset(\"../data_preprocessed/ml-100k/data_split/test_set_leave_one.json\")\n",
    "movie_title_to_id = map_title_to_id(\"../data/ml-100k/movies.dat\")\n",
    "\n",
    "train_data = convert_titles_to_ids(train_data, movie_title_to_id)\n",
    "valid_data = convert_titles_to_ids(valid_data, movie_title_to_id)\n",
    "test_data = convert_titles_to_ids(test_data, movie_title_to_id)\n",
    "\n",
    "train_matrix, actual_list_val, actual_list_test = create_train_matrix_and_actual_lists(train_data, valid_data,\n",
    "                                                                                        test_data, movie_title_to_id)\n",
    "train_matrix = csr_matrix(train_matrix)  # Convert train_matrix to a CSR matrix\n",
    "\n",
    "\n",
    "# 2. Model Creation\n",
    "num_users, num_items = train_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emilianopenaloza/Git/LLM4Rec/notebooks/../model/MF.py:11: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(m.weight)\n"
     ]
    }
   ],
   "source": [
    "args = parse_args(True)\n",
    "user_embedder = decoderAttention(embedding_dim,num_heads,num_layers,output_emb, 0  )\n",
    "\n",
    "model = MatrixFactorizationLLM(num_users, user_embedder,num_items, args).to(args.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings_true = np.load(model_rankings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MatrixFactorizationLLM(\n",
       "  (user_embeddings): decoderAttention(\n",
       "    (attnModule): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (mlp_shrinker): decoderMLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=448, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.25, inplace=False)\n",
       "        (3): Linear(in_features=448, out_features=128, bias=True)\n",
       "        (4): ReLU()\n",
       "        (5): Dropout(p=0.25, inplace=False)\n",
       "        (6): Identity()\n",
       "        (7): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (8): Dropout(p=0.25, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (item_embeddings): Embedding(1682, 64)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_path,map_location=torch.device('mps')))\n",
    "user_embedder.load_state_dict(torch.load(user_embeddings_path,map_location=torch.device('mps')))\n",
    "model.user_embeddings = user_embedder\n",
    "\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = {\n",
    "    \"Comedy\": \"Summary: This list of comedy movies includes adventure, drama, science fiction, and romance. Set in various locations, these films portray the trials, tribulations, and humorous situations of diverse characters in different settings.\",\n",
    "    \"Romance\": \"Summary: A collection of lighthearted romantic comedies and dramas set in various locations and time periods, showcasing diverse relationships and the challenges they face.\",\n",
    "    \"Drama\": \"Summary: A collection of drama films from the 1990s, exploring various themes such as historical mysteries, trials and tribulations of life in poverty-stricken neighborhoods, LGBT-related stories, romantic comedies, biographical tales, and epic journeys.\",\n",
    "    \"Action\": \"Summary: Action movies with a dark and gritty tone, featuring intense crime stories and strong performances from the cast.\"\n",
    "    }\n",
    "  \n",
    "  \n",
    "def get_preds(summaries,top_k = 20 ): \n",
    "    args.embedding_module = 't5'\n",
    "    embs = get_genrewise_embeddings(summaries,args, model= True)\n",
    "    genre_list = get_genres()\n",
    "    embs_tens = model.user_embeddings.prepare_input(embs,genre_list).to(args.device)\n",
    "\n",
    "    scores = model.predict(embs_tens.unsqueeze(0)).cpu().detach().numpy()\n",
    "    ranked_items = np.argsort(scores)[::-1]\n",
    "    ranked_items = list(reversed(ranked_items[0] +1))\n",
    "\n",
    "    recall_val = recall_at_k_one(actual_list_val[3], ranked_items, 20)\n",
    "    print(f\"checking recall: {recall_val=}\")\n",
    "    reversed_movie_title_to_id = {v: k for k, v in movie_title_to_id.items()}\n",
    "    movie_titles_ranked = [f'{index} : {reversed_movie_title_to_id[i+1]} {id_genre_map[i+1]}' for index,i in enumerate(ranked_items[:20])]\n",
    "    pp(movie_titles_ranked)\n",
    "    return ranked_items[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "ranked_items=[560, 786, 58, 1596, 53, 696, 790, 12, 825, 730, 1, 1421, 1041, 1551, 274, 963, 782, 547, 783, 724, 789, 785, 1518, 374, 787, 1239, 958, 1042, 764, 1148, 1208, 276, 1110, 23, 1157, 1073, 1407, 317, 1058, 52, 1209, 81, 1068, 1043, 1098, 54, 1167, 735, 549, 1195, 1409, 1220, 697, 363, 1153, 1408, 1354, 406, 398, 1297, 28, 458, 1268, 1426, 86, 1305, 557, 1519, 47, 591, 376, 1186, 562, 49, 1179, 365, 57, 78, 739, 478, 1627, 129, 1227, 1072, 727, 843, 1066, 1230, 956, 1112, 1425, 62, 1595, 464, 1443, 788, 1004, 1423, 1101, 419, 1312, 1621, 931, 416, 1063, 1474, 1480, 386, 110, 833, 364, 382, 1210, 791, 147, 1637, 5, 64, 545, 88, 548, 378, 997, 1185, 614, 67, 842, 135, 380, 2, 699, 66, 1337, 606, 1471, 404, 1040, 932, 173, 381, 771, 1115, 356, 1328, 1438, 1351, 1355, 397, 51, 540, 609, 700, 1287, 1052, 598, 469, 793, 979, 1503, 781, 120, 489, 1269, 1458, 717, 1495, 765, 588, 607, 775, 1017, 1577, 1396, 1194, 566, 43, 70, 584, 131, 383, 718, 564, 692, 495, 166, 622, 602, 1470, 79, 1451, 417, 574, 1028, 21, 415, 38, 693, 1441, 1505, 625, 387, 465, 792, 1223, 17, 773, 778, 156, 1552, 4, 1314, 720, 1070, 604, 1303, 744, 1147, 26, 36, 1517, 725, 1440, 1071, 42, 318, 836, 1169, 371, 213, 694, 1530, 1445, 1424, 1604, 715, 150, 952, 924, 731, 418, 1049, 139, 950, 146, 202, 723, 35, 593, 1446, 982, 480, 1442, 636, 525, 1217, 40, 1464, 940, 1557, 1020, 1079, 1253, 721, 1368, 808, 1218, 1121, 1509, 490, 946, 1177, 41, 29, 776, 373, 577, 279, 1523, 493, 569, 608, 594, 955, 628, 384, 1550, 1100, 39, 1450, 1205, 273, 770, 962, 648, 479, 1069, 407, 582, 396, 807, 85, 1207, 519, 821, 517, 518, 1145, 1135, 844, 153, 1228, 160, 832, 1603, 661, 82, 761, 943, 372, 542, 524, 275, 1095, 621, 504, 603, 1311, 926, 966, 487, 953, 980, 208, 375, 976, 395, 1475, 141, 756, 63, 207, 550, 804, 237, 1213, 106, 1655, 1478, 1524, 1325, 1469, 370, 1134, 423, 475, 1097, 1221, 1000, 498, 118, 366, 1675, 1472, 367, 742, 65, 161, 1672, 1502, 148, 175, 1113, 95, 572, 425, 998, 546, 823, 325, 1126, 134, 1632, 618, 481, 772, 716, 736, 977, 759, 389, 1224, 76, 485, 1131, 1436, 68, 1196, 1614, 728, 200, 31, 103, 928, 800, 287, 1467, 34, 1180, 639, 1510, 1529, 456, 87, 1479, 1628, 1267, 332, 555, 965, 684, 1489, 1053, 583, 652, 1447, 155, 1162, 164, 1416, 1078, 933, 412, 1139, 500, 394, 1273, 1468, 1544, 959, 631, 1124, 212, 981, 483, 1531, 703, 229, 319, 194, 1184, 508, 758, 1540, 1533, 1597, 1564, 974, 27, 794, 1298, 1212, 529, 553, 492, 137, 108, 1032, 1045, 368, 1254, 972, 1076, 385, 210, 1246, 1037, 331, 818, 726, 1140, 105, 698, 250, 722, 1584, 1151, 638, 516, 96, 3, 73, 1515, 655, 686, 499, 72, 713, 1031, 1270, 1074, 1452, 1288, 951, 1512, 1030, 111, 56, 432, 925, 238, 1382, 1361, 805, 1401, 486, 13, 1214, 1173, 285, 999, 205, 140, 233, 1109, 777, 69, 1622, 766, 138, 650, 733, 470, 1055, 1369, 1211, 1453, 822, 280, 1187, 414, 215, 810, 391, 613, 184, 579, 554, 1044, 712, 1084, 1064, 452, 947, 471, 124, 1114, 236, 819, 15, 80, 964, 779, 769, 358, 503, 1422, 575, 806, 393, 1188, 1136, 1197, 1437, 610, 629, 1008, 809, 854, 1202, 1275, 960, 219, 975, 1357, 1093, 1203, 820, 704, 392, 1219, 668, 526, 1562, 581, 796, 829, 623, 506, 1299, 1652, 737, 77, 1444, 83, 239, 1333, 1521, 626, 1204, 1231, 541, 611, 811, 497, 1271, 1200, 1289, 187, 467, 441, 755, 16, 527, 1172, 1545, 1181, 195, 1291, 107, 620, 48, 734, 1411, 97, 967, 944, 1266, 745, 1009, 1057, 445, 1539, 856, 472, 523, 230, 996, 231, 232, 1107, 877, 171, 654, 1132, 1465, 1146, 149, 797, 463, 1282, 669, 1092, 1199, 328, 74, 624, 1284, 663, 1522, 369, 1406, 531, 71, 558, 1488, 774, 466, 1123, 133, 429, 847, 543, 941, 1075, 1541, 855, 845, 1487, 858, 1506, 33, 1625, 293, 670, 1367, 388, 1457, 647, 763, 113, 978, 209, 590, 1034, 152, 520, 1370, 875, 1413, 1164, 1168, 220, 1326, 261, 1281, 203, 1001, 109, 453, 1189, 1285, 922, 760, 1250, 1082, 509, 459, 801, 20, 1417, 496, 357, 90, 1215, 674, 473, 537, 165, 288, 1415, 93, 991, 102, 1493, 1035, 183, 1249, 192, 1046, 91, 1033, 402, 1170, 1611, 738, 732, 9, 954, 10, 816, 1036, 1629, 265, 143, 159, 1397, 1150, 1485, 635, 685, 1462, 995, 714, 989, 827, 1454, 223, 1190, 551, 1276, 1496, 116, 633, 740, 300, 167, 831, 186, 864, 1532, 410, 443, 886, 1047, 447, 848, 536, 1319, 302, 327, 1638, 530, 277, 815, 1459, 672, 1067, 949, 658, 221, 145, 224, 1392, 1394, 427, 1547, 1120, 511, 1414, 188, 6, 286, 1300, 695, 158, 125, 741, 930, 1336, 826, 528, 405, 1307, 437, 682, 828, 1182, 193, 89, 191, 835, 1565, 710, 983, 1262, 420, 1528, 802, 1108, 235, 799, 291, 1362, 1554, 656, 1332, 1116, 920, 92, 1658, 440, 651, 1661, 768, 324, 126, 222, 399, 1490, 1278, 576, 872, 1309, 32, 709, 803, 653, 234, 1149, 616, 1643, 400, 1327, 204, 1295, 199, 1174, 123, 568, 1585, 1085, 127, 226, 1159, 840, 1261, 795, 94, 1039, 1439, 477, 227, 289, 144, 1263, 1014, 482, 1206, 1310, 1538, 1653, 595, 281, 767, 502, 180, 501, 866, 615, 409, 270, 1117, 1664, 100, 448, 322, 1163, 657, 1555, 460, 292, 247, 945, 1183, 867, 649, 601, 702, 846, 596, 1593, 22, 597, 178, 310, 507, 1161, 413, 936, 676, 630, 154, 1308, 1516, 681, 1598, 433, 513, 1048, 430, 1125, 278, 1277, 99, 514, 1061, 1383, 1376, 849, 401, 75, 294, 862, 436, 295, 1448, 642, 197, 641, 1616, 585, 206, 1343, 1375, 1659, 1600, 136, 812, 1680, 1334, 1059, 1094, 1091, 1011, 586, 1165, 179, 619, 1152, 841, 491, 271, 990, 747, 874, 1244, 428, 746, 403, 1232, 142, 971, 934, 986, 1642, 282, 556, 970, 1155, 1500, 535, 259, 434, 664, 438, 1341, 948, 1381, 1380, 176, 813, 659, 859, 190, 706, 1481, 688, 245, 675, 1317, 1225, 248, 25, 196, 680, 260, 446, 645, 1051, 244, 290, 1435, 1247, 162, 1617, 450, 339, 929, 1007, 299, 308, 323, 494, 612, 888, 320, 338, 1654, 1138, 1418, 1010, 837, 283, 119, 1587, 1455, 571, 268, 824, 218, 869, 1216, 1374, 660, 882, 1609, 1142, 708, 1378, 798, 1089, 749, 1302, 439, 1119, 1610, 457, 1511, 534, 335, 1088, 1534, 1476, 605, 1054, 1160, 879, 880, 889, 435, 1574, 284, 262, 1002, 1255, 1520, 1665, 422, 361, 1016, 748, 1083, 850, 340, 334, 249, 454, 1605, 687, 321, 1013, 870, 476, 873, 1023, 1315, 881, 269, 1006, 935, 1377, 337, 757, 1077, 1025, 246, 578, 1379, 1241, 1141, 1566, 1156, 122, 1144, 1015, 1676, 957, 993, 1363, 885, 303, 1346, 411, 329, 533, 587, 1608, 865, 1499, 752, 189, 484, 678, 1293, 1283, 1647, 890, 1258, 589, 883, 1486, 887, 55, 1660, 272, 101, 1537, 1242, 968, 1106, 359, 305, 264, 915, 354, 1304, 1395, 377, 923, 1065, 341, 1222, 263, 1286, 1245, 1316, 538, 1257, 347, 121, 336, 301, 1364, 201, 1080, 1347, 570, 307, 921, 112, 351, 1461, 1568, 1607, 559, 897, 1356, 839, 1430, 1626, 1412, 1129, 306, 298, 11, 1087, 1005, 174, 1582, 348, 969, 1128, 1569, 130, 1682, 1127, 899, 343, 988, 1669, 1580, 1427, 181, 515, 1143, 910, 719, 177, 1235, 1279, 421, 830, 961, 1572, 1021, 863, 342, 349, 1673, 1633, 1296, 1589, 46, 1599, 851, 7, 296, 938, 1390, 474, 1338, 256, 1456, 1373, 1388, 1137, 707, 1192, 691, 1410, 426, 1365, 1240, 992, 1158, 711, 168, 900, 1663, 1301, 1567, 1657, 1498, 241, 896, 50, 242, 297, 690, 1399, 258, 316, 251, 1526, 1027, 871, 1349, 1463, 266, 632, 214, 861, 904, 644, 627, 1081, 994, 1543, 1578, 743, 1389, 1060, 1340, 1466, 1050, 1404, 1420, 1644, 1668, 573, 243, 24, 172, 522, 312, 1096, 1306, 1674, 689, 1656, 1387, 1546, 257, 1386, 1198, 1252, 198, 1432, 1576, 151, 1477, 488, 1648, 379, 973, 1434, 634, 898, 1491, 1482, 637, 1649, 919, 326, 1237, 84, 1238, 780, 254, 461, 1620, 1118, 852, 985, 1483, 1350, 1514, 942, 1233, 1591, 1492, 311, 1662, 1348, 333, 902, 1175, 114, 917, 1154, 561, 104, 1497, 344, 679, 817, 1590, 117, 1243, 1313, 44, 580, 1433, 646, 330, 1090, 505, 1402, 903, 132, 1513, 169, 1272, 521, 1508, 1062, 1666, 1645, 1460, 1393, 1594, 216, 853, 240, 352, 1264, 1400, 1558, 444, 442, 671, 1419, 1274, 683, 1111, 252, 267, 1646, 1504, 1592, 1329, 1618, 353, 1525, 37, 431, 838, 939, 892, 729, 539, 677, 893, 673, 1324, 909, 1236, 860, 1099, 1259, 984, 544, 462, 19, 1507, 1606, 1056, 1650, 1623, 14, 592, 1226, 1536, 665, 345, 1234, 868, 567, 1586, 1636, 1613, 1583, 891, 640, 1494, 1588, 662, 1571, 1086, 1331, 455, 1549, 1320, 878, 1542, 916, 309, 1171, 1318, 666, 1323, 987, 1405, 255, 1344, 1429, 451, 390, 1178, 1681, 1012, 1449, 1248, 1371, 753, 1679, 1670, 1290, 1166, 600, 1133, 1556, 762, 927, 1353, 228, 1559, 906, 1256, 1501, 1640, 1251, 1431, 304, 315, 1292, 1105, 1570, 1024, 1122, 512, 563, 408, 1130, 1321, 468, 1345, 834, 360, 350, 1360, 98, 750, 918, 1193, 1391, 552, 1581, 1191, 1372, 937, 884, 1624, 814, 894, 510, 157, 1553, 905, 217, 907, 1634, 362, 424, 1339, 1641, 1018, 911, 163, 61, 170, 346, 1294, 185, 1398, 1385, 1366, 617, 1612, 1019, 1265, 45, 1384, 1280, 1527, 30, 1330, 565, 1635, 1403, 701, 914, 1029, 1342, 705, 1631, 1003, 1022, 1535, 1667, 1103, 8, 1229, 667, 876, 1352, 314, 211, 784, 60, 449, 532, 1102, 1104, 1677, 1573, 253, 18, 857, 182, 1176, 313, 1428, 1563, 1548, 1601, 1473, 1575, 1038, 1615, 1359, 225, 1671, 59, 1260, 1322, 1026, 1358, 1201, 643, 1561, 895, 901, 1619, 1484, 908, 1639, 128, 1630, 751, 355, 912, 913, 599, 1560, 1651, 115, 754, 1335, 1678, 1579, 1602]\n",
      "checking recall: recall_val=0.0\n",
      "[\"0 : Mary Shelley's Frankenstein (1994) Drama|Horror\",\n",
      " '1 : Roommates (1995) Comedy|Drama',\n",
      " '2 : Three Colors: Red (1994) Drama',\n",
      " '3 : Romper Stomper (1992) Action|Drama',\n",
      " '4 : Outbreak (1995) Action|Drama|Thriller',\n",
      " '5 : Basketball Diaries, The (1995) Drama',\n",
      " \"6 : Baby-Sitters Club, The (1995) Children's\",\n",
      " '7 : Mighty Aphrodite (1995) Comedy',\n",
      " '8 : Phantom, The (1996) Adventure',\n",
      " '9 : Corrina, Corrina (1994) Comedy|Drama|Romance',\n",
      " '10 : GoldenEye (1995) Action|Adventure|Thriller',\n",
      " '11 : Suture (1993) Film-Noir|Thriller',\n",
      " '12 : Just Cause (1995) Mystery|Thriller',\n",
      " '13 : Hunted, The (1995) Action',\n",
      " '14 : Sense and Sensibility (1995) Drama|Romance',\n",
      " '15 : Month by the Lake, A (1995) Comedy|Drama',\n",
      " '16 : Milk Money (1994) Comedy|Romance',\n",
      " \"17 : NeverEnding Story III, The (1994) Children's|Fantasy\",\n",
      " '18 : Beyond Bedlam (1993) Drama|Horror',\n",
      " '19 : Exit to Eden (1994) Comedy']\n"
     ]
    }
   ],
   "source": [
    "get_preds(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "ranked_items=[58, 560, 1596, 786, 1421, 730, 53, 790, 782, 785, 1551, 789, 958, 787, 696, 562, 1195, 52, 825, 1073, 1407, 1043, 1058, 1297, 54, 1148, 1110, 1208, 557, 963, 1518, 1268, 1157, 1209, 1153, 788, 783, 57, 67, 1167, 1220, 1230, 478, 1041, 1408, 86, 458, 1354, 1239, 12, 1305, 382, 1426, 1, 1068, 1519, 1409, 1269, 724, 1101, 931, 464, 62, 833, 1112, 274, 979, 317, 88, 997, 1063, 1637, 110, 1505, 66, 28, 1423, 1042, 735, 1424, 602, 276, 81, 380, 364, 699, 23, 547, 131, 1312, 374, 356, 1210, 614, 1098, 793, 1337, 213, 363, 932, 383, 2, 566, 1186, 51, 598, 982, 1072, 1438, 1474, 1020, 1115, 1627, 697, 1503, 1443, 791, 381, 764, 378, 146, 79, 1179, 778, 549, 465, 49, 479, 781, 480, 1675, 1004, 731, 604, 418, 792, 406, 727, 202, 564, 1445, 365, 129, 836, 1621, 1552, 821, 548, 173, 1303, 237, 1147, 376, 744, 956, 415, 591, 715, 469, 64, 1328, 540, 832, 212, 625, 843, 398, 78, 1227, 652, 1066, 545, 47, 273, 693, 135, 584, 1355, 325, 164, 21, 65, 1314, 524, 156, 1480, 416, 609, 456, 794, 1121, 1425, 1495, 1604, 622, 1451, 396, 603, 1595, 924, 739, 636, 1470, 1185, 1180, 134, 1557, 550, 808, 419, 700, 70, 574, 516, 481, 1040, 485, 582, 194, 842, 1458, 85, 404, 1017, 980, 519, 493, 5, 1223, 166, 1071, 1351, 1447, 141, 36, 394, 318, 287, 943, 1069, 372, 804, 423, 425, 1446, 139, 487, 495, 1213, 397, 1464, 650, 150, 56, 17, 489, 34, 43, 250, 965, 87, 386, 1523, 962, 606, 628, 807, 35, 525, 1452, 236, 1467, 1530, 1287, 1194, 38, 1368, 39, 238, 147, 648, 504, 483, 4, 1224, 728, 1145, 417, 1298, 953, 1000, 577, 712, 1603, 490, 823, 42, 569, 1037, 655, 395, 588, 1231, 118, 1135, 499, 1442, 1471, 977, 138, 1632, 771, 716, 452, 527, 82, 946, 1052, 1510, 1139, 106, 68, 1396, 1131, 721, 1382, 998, 736, 553, 607, 952, 1577, 137, 1472, 940, 215, 1369, 1055, 1509, 1028, 1169, 208, 1217, 279, 518, 1126, 1440, 358, 171, 407, 1221, 120, 1517, 844, 1441, 593, 933, 594, 1540, 63, 229, 219, 626, 717, 1205, 1070, 1049, 720, 1188, 775, 818, 371, 475, 239, 3, 854, 639, 974, 108, 964, 773, 1196, 725, 661, 1416, 384, 765, 387, 542, 694, 1079, 153, 1207, 972, 608, 529, 631, 981, 621, 498, 686, 205, 819, 1134, 1177, 1672, 517, 995, 1100, 1417, 432, 1597, 618, 624, 332, 207, 1533, 200, 375, 1479, 806, 1450, 203, 947, 1270, 776, 926, 1325, 175, 233, 684, 1214, 581, 951, 950, 1401, 1173, 261, 1436, 486, 40, 160, 1008, 463, 928, 692, 331, 161, 373, 1253, 1190, 1172, 623, 223, 155, 503, 583, 41, 674, 29, 610, 723, 654, 232, 955, 1124, 508, 809, 877, 742, 1289, 1064, 414, 772, 148, 412, 638, 103, 810, 537, 96, 1254, 976, 1652, 453, 718, 1136, 1521, 1203, 302, 1284, 805, 1512, 944, 1266, 1246, 105, 1453, 1095, 357, 698, 613, 663, 531, 875, 1288, 1031, 864, 13, 500, 26, 1057, 734, 210, 761, 1444, 1422, 391, 293, 441, 1614, 672, 922, 959, 822, 472, 590, 506, 1655, 1218, 107, 770, 265, 1114, 220, 270, 275, 133, 1271, 886, 1625, 367, 710, 579, 1370, 536, 1189, 471, 447, 231, 966, 1074, 393, 1299, 991, 319, 756, 1469, 611, 1550, 526, 328, 629, 1078, 1140, 140, 111, 300, 1489, 1361, 1044, 370, 1326, 149, 777, 1097, 620, 1162, 558, 1394, 1584, 1658, 368, 1084, 1564, 713, 1454, 682, 766, 1282, 1545, 511, 1076, 327, 492, 1524, 1202, 473, 90, 520, 745, 975, 27, 1611, 1045, 1035, 1528, 83, 800, 528, 847, 445, 815, 1092, 1034, 967, 685, 209, 230, 324, 221, 74, 224, 925, 668, 1120, 385, 1357, 658, 1197, 1113, 235, 429, 855, 165, 1478, 1053, 669, 389, 801, 1295, 76, 259, 676, 733, 285, 1232, 392, 1502, 703, 1278, 1565, 195, 1392, 1311, 1181, 1562, 601, 428, 555, 1030, 872, 1163, 1457, 366, 740, 835, 1529, 1082, 763, 1506, 234, 633, 1014, 496, 1211, 443, 1151, 983, 1094, 73, 470, 858, 6, 467, 430, 1164, 1146, 1046, 497, 260, 856, 758, 1273, 1654, 1411, 77, 1263, 1036, 437, 945, 862, 635, 1228, 1123, 615, 1680, 226, 1383, 1244, 572, 100, 184, 1485, 1184, 651, 291, 816, 80, 310, 339, 1187, 126, 714, 656, 97, 183, 1468, 322, 1281, 48, 737, 227, 1598, 1593, 136, 681, 1150, 1617, 292, 1367, 482, 1496, 95, 402, 92, 1032, 280, 338, 1275, 546, 1333, 436, 554, 507, 154, 1541, 888, 1628, 438, 649, 874, 218, 759, 849, 1488, 616, 440, 796, 1475, 405, 222, 308, 820, 124, 1459, 187, 1341, 930, 448, 989, 1093, 1250, 271, 535, 178, 1465, 595, 934, 688, 1515, 162, 889, 1531, 726, 509, 1276, 704, 1267, 523, 176, 960, 248, 1010, 1418, 1462, 1132, 109, 722, 769, 1212, 159, 1182, 9, 1585, 1376, 450, 1332, 859, 152, 680, 811, 999, 1165, 882, 949, 732, 33, 123, 645, 294, 1487, 575, 199, 127, 1587, 1170, 1362, 675, 1414, 1308, 954, 1152, 247, 268, 866, 978, 1327, 102, 802, 827, 829, 197, 477, 1380, 1310, 245, 289, 1007, 446, 848, 1204, 1009, 1538, 15, 1160, 69, 846, 1051, 299, 1544, 1013, 936, 1083, 1168, 541, 1085, 91, 335, 576, 31, 99, 1161, 993, 1661, 1199, 1554, 873, 870, 1285, 1638, 1435, 278, 32, 1415, 288, 1511, 513, 1547, 1109, 1200, 1249, 457, 1061, 286, 466, 204, 340, 664, 321, 1006, 1664, 845, 1609, 188, 1033, 1262, 957, 439, 1629, 1047, 1307, 840, 1215, 1448, 779, 850, 746, 1159, 113, 879, 687, 630, 144, 1643, 334, 1346, 647, 1088, 1023, 1406, 1183, 986, 502, 1676, 587, 1107, 1315, 551, 642, 578, 320, 71, 337, 1455, 1119, 659, 1206, 619, 747, 1610, 1378, 341, 435, 1534, 1156, 1089, 272, 797, 1486, 1642, 586, 196, 351, 55, 1174, 867, 142, 1293, 968, 869, 72, 420, 359, 1499, 1039, 1291, 1363, 1622, 1091, 948, 361, 246, 1653, 760, 1381, 1225, 459, 1002, 1379, 1077, 1374, 708, 748, 1015, 284, 1011, 295, 605, 1149, 1319, 388, 1364, 186, 1605, 1493, 837, 935, 323, 1574, 795, 841, 543, 427, 244, 1539, 191, 1375, 1600, 971, 970, 887, 290, 476, 1001, 880, 1437, 422, 303, 1626, 1569, 1283, 283, 1242, 454, 1144, 883, 941, 20, 670, 538, 1608, 1316, 119, 1566, 1021, 921, 1607, 1258, 167, 897, 193, 1572, 1219, 597, 1155, 767, 1255, 749, 969, 839, 1377, 403, 1080, 1129, 1427, 379, 1302, 1016, 530, 158, 122, 830, 201, 89, 774, 568, 755, 307, 881, 130, 426, 347, 305, 1247, 1286, 354, 434, 752, 1476, 660, 917, 501, 11, 316, 1567, 1395, 1025, 988, 1216, 101, 277, 116, 75, 813, 861, 1304, 1142, 269, 757, 433, 1300, 719, 589, 112, 1500, 890, 329, 904, 1050, 281, 1477, 915, 46, 1580, 369, 179, 413, 534, 992, 343, 1087, 1279, 1238, 1410, 910, 125, 145, 410, 206, 190, 1245, 7, 306, 1526, 1343, 349, 192, 181, 799, 1226, 515, 559, 342, 627, 421, 1309, 885, 1106, 996, 474, 1048, 923, 22, 828, 121, 198, 1240, 1665, 514, 177, 1461, 1660, 1682, 1399, 961, 973, 1306, 1647, 172, 1252, 442, 1127, 1674, 24, 249, 264, 1222, 691, 241, 1065, 1537, 240, 653, 348, 296, 1117, 262, 1412, 411, 1413, 1264, 44, 871, 301, 1241, 1356, 1532, 336, 1096, 1456, 831, 174, 689, 1318, 84, 1644, 1347, 863, 678, 657, 632, 1116, 865, 1235, 242, 104, 298, 1589, 256, 706, 892, 1432, 216, 1388, 1060, 1125, 1633, 505, 900, 939, 743, 1081, 258, 741, 1143, 10, 1108, 1390, 1430, 1568, 377, 1373, 1591, 1141, 1543, 282, 189, 1067, 151, 580, 1466, 257, 1192, 780, 898, 1137, 1582, 1350, 1656, 312, 1558, 1154, 1371, 803, 352, 851, 1397, 254, 1439, 671, 985, 1344, 333, 1331, 117, 711, 707, 1546, 1340, 1599, 1056, 753, 1620, 1618, 984, 896, 1257, 1138, 1338, 1005, 345, 1158, 462, 1434, 455, 1400, 1027, 409, 644, 114, 93, 488, 263, 1578, 168, 1669, 1649, 1386, 1646, 990, 1301, 1522, 461, 1128, 1668, 143, 1482, 826, 539, 50, 1657, 1118, 665, 267, 533, 1501, 1296, 311, 1387, 1570, 1111, 266, 251, 19, 641, 326, 1590, 899, 1404, 570, 1099, 1497, 1365, 1516, 987, 1348, 702, 573, 1663, 1259, 1648, 916, 1673, 1576, 1640, 860, 1336, 634, 353, 1277, 1612, 1606, 1198, 297, 214, 1313, 994, 180, 561, 522, 679, 1520, 243, 1236, 1393, 918, 646, 552, 1662, 690, 938, 1389, 94, 1429, 1105, 1650, 182, 1681, 891, 1460, 1090, 1178, 1679, 1234, 1256, 1334, 1514, 1670, 592, 1324, 1248, 1555, 683, 1175, 451, 1329, 929, 1419, 362, 1483, 1498, 1491, 919, 902, 729, 1659, 1349, 431, 1525, 330, 1280, 1508, 563, 1463, 1504, 494, 853, 510, 1166, 868, 1592, 838, 400, 1636, 1251, 596, 344, 637, 1494, 662, 8, 1594, 640, 1449, 45, 893, 571, 255, 834, 169, 25, 1588, 878, 1402, 1024, 484, 1513, 1353, 1102, 1339, 98, 1583, 942, 1631, 1549, 852, 132, 1320, 1527, 491, 1345, 444, 1294, 1433, 460, 390, 399, 768, 1507, 1536, 903, 1265, 585, 1292, 701, 1634, 1243, 1556, 909, 315, 1352, 401, 1431, 37, 1323, 1492, 1372, 170, 1018, 914, 1420, 1553, 556, 884, 1586, 1613, 16, 521, 1122, 705, 1535, 1385, 1677, 163, 1342, 784, 1086, 666, 876, 1360, 468, 309, 906, 1171, 817, 1542, 1405, 350, 1624, 1490, 225, 677, 894, 1619, 1038, 814, 565, 1019, 1133, 1559, 709, 228, 1237, 252, 360, 211, 61, 600, 14, 1623, 346, 512, 1229, 1359, 927, 18, 1062, 1645, 1666, 907, 1581, 1054, 1261, 1548, 1403, 905, 1641, 1130, 1193, 1075, 1571, 1615, 544, 1398, 1473, 1563, 532, 617, 1274, 1630, 1601, 304, 59, 253, 1233, 157, 313, 1191, 30, 824, 1575, 750, 1384, 1272, 408, 812, 1330, 857, 895, 901, 937, 911, 667, 1484, 1103, 1029, 762, 738, 920, 1671, 1391, 449, 1104, 1026, 751, 1003, 1561, 424, 1176, 1616, 673, 1428, 60, 567, 1290, 1022, 217, 1059, 912, 599, 1322, 908, 314, 185, 643, 1260, 1321, 1667, 1201, 1639, 612, 1635, 1573, 1366, 115, 695, 1678, 1358, 1335, 128, 913, 355, 1317, 1012, 1560, 1481, 1579, 1651, 754, 1602, 798]\n",
      "checking recall: recall_val=0.5\n",
      "['0 : Three Colors: Red (1994) Drama',\n",
      " \"1 : Mary Shelley's Frankenstein (1994) Drama|Horror\",\n",
      " '2 : Romper Stomper (1992) Action|Drama',\n",
      " '3 : Roommates (1995) Comedy|Drama',\n",
      " '4 : Suture (1993) Film-Noir|Thriller',\n",
      " '5 : Corrina, Corrina (1994) Comedy|Drama|Romance',\n",
      " '6 : Outbreak (1995) Action|Drama|Thriller',\n",
      " \"7 : Baby-Sitters Club, The (1995) Children's\",\n",
      " '8 : Milk Money (1994) Comedy|Romance',\n",
      " '9 : Perez Family, The (1995) Comedy|Romance',\n",
      " '10 : Hunted, The (1995) Action',\n",
      " '11 : Tommy Boy (1995) Comedy',\n",
      " '12 : Dazed and Confused (1993) Comedy',\n",
      " '13 : Relative Fear (1994) Horror|Thriller',\n",
      " '14 : Basketball Diaries, The (1995) Drama',\n",
      " \"15 : Stephen King's The Langoliers (1995) Horror\",\n",
      " '16 : Savage Nights (Nuits fauves, Les) (1992) Drama',\n",
      " '17 : Natural Born Killers (1994) Action|Thriller',\n",
      " '18 : Phantom, The (1996) Adventure',\n",
      " '19 : Reality Bites (1994) Comedy|Drama']\n"
     ]
    }
   ],
   "source": [
    "summaries = {\n",
    "    \"Children\": \"Summary: animated toy adventures.\",  \n",
    "    }\n",
    "  \n",
    "get_preds(summaries)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "ranked_items=[58, 560, 1596, 790, 1421, 786, 53, 730, 782, 1551, 785, 1148, 787, 789, 958, 54, 1073, 562, 52, 1043, 825, 1407, 557, 1110, 1058, 1195, 1297, 963, 1153, 1208, 1157, 1518, 57, 1268, 696, 788, 1167, 67, 1230, 1354, 478, 1220, 1426, 783, 1408, 1305, 382, 931, 833, 1269, 979, 1209, 1112, 1041, 1063, 1637, 458, 464, 1068, 66, 1409, 997, 380, 1101, 110, 62, 1423, 1239, 1424, 88, 1312, 602, 793, 699, 86, 1519, 932, 28, 317, 1505, 614, 1, 1042, 356, 1072, 383, 364, 131, 81, 274, 735, 566, 724, 12, 146, 1337, 1438, 982, 51, 276, 1020, 23, 1210, 480, 213, 381, 547, 1098, 792, 1627, 697, 1115, 1503, 598, 564, 1474, 479, 1445, 604, 731, 202, 2, 378, 791, 1186, 465, 1675, 79, 778, 821, 781, 363, 418, 173, 832, 49, 764, 237, 1179, 1004, 1621, 548, 652, 836, 1147, 625, 1443, 374, 212, 727, 406, 524, 744, 129, 1552, 415, 843, 376, 1303, 545, 325, 1480, 365, 456, 164, 469, 591, 693, 1227, 1314, 540, 1328, 135, 794, 980, 549, 956, 273, 78, 519, 1495, 622, 603, 416, 1066, 65, 1180, 481, 1470, 156, 47, 287, 1017, 1604, 1355, 398, 1458, 425, 21, 700, 842, 609, 56, 166, 1451, 194, 650, 493, 396, 487, 636, 397, 516, 1351, 495, 485, 1425, 739, 238, 1287, 423, 606, 1069, 250, 1298, 655, 965, 648, 139, 715, 483, 419, 236, 490, 1185, 525, 712, 417, 1231, 736, 1368, 404, 588, 452, 628, 147, 68, 584, 229, 728, 1126, 1603, 518, 137, 489, 1509, 1369, 386, 1510, 1145, 150, 215, 504, 1037, 1135, 527, 1540, 1382, 208, 141, 946, 1452, 1530, 358, 171, 1173, 1188, 1079, 981, 661, 1194, 933, 1396, 1169, 205, 43, 1471, 372, 134, 631, 594, 593, 550, 233, 219, 621, 1055, 1472, 844, 332, 626, 64, 1577, 854, 207, 940, 529, 3, 4, 1028, 1205, 1517, 160, 607, 120, 765, 503, 924, 200, 407, 239, 639, 947, 1134, 1440, 1049, 384, 175, 618, 972, 138, 155, 232, 1213, 610, 1124, 499, 995, 441, 1008, 638, 1172, 261, 1254, 432, 1450, 716, 463, 1557, 877, 654, 1190, 34, 582, 1447, 486, 1652, 223, 371, 302, 153, 537, 387, 672, 331, 394, 624, 517, 1064, 663, 148, 864, 35, 531, 5, 453, 608, 70, 412, 943, 1040, 742, 1625, 1000, 1523, 823, 574, 1121, 1284, 42, 447, 1417, 804, 133, 569, 977, 886, 508, 1595, 964, 674, 629, 1632, 265, 39, 682, 210, 231, 1394, 808, 87, 1370, 1095, 1467, 1672, 810, 500, 526, 318, 36, 875, 1071, 270, 1223, 220, 966, 1611, 209, 710, 328, 807, 818, 118, 1658, 445, 106, 85, 203, 558, 1162, 17, 230, 553, 1446, 1052, 38, 221, 991, 952, 293, 327, 676, 224, 235, 1464, 1597, 140, 1082, 1453, 974, 855, 745, 498, 234, 475, 428, 63, 319, 1232, 357, 536, 872, 1139, 1489, 835, 395, 953, 962, 1224, 601, 613, 668, 1654, 1221, 161, 776, 1680, 669, 951, 1422, 1562, 226, 149, 1528, 300, 506, 430, 925, 1120, 1524, 862, 1189, 1014, 429, 763, 623, 858, 1282, 998, 1217, 437, 1084, 1136, 800, 443, 950, 1441, 520, 259, 1584, 983, 260, 324, 82, 1177, 1131, 279, 542, 819, 681, 1617, 1289, 1392, 1479, 438, 436, 1244, 1295, 721, 1401, 1202, 96, 523, 1113, 1163, 856, 766, 105, 1164, 928, 227, 154, 1454, 339, 658, 218, 222, 1357, 888, 74, 922, 414, 577, 1092, 1512, 945, 579, 620, 635, 1367, 275, 723, 1614, 482, 73, 338, 934, 822, 440, 1383, 1444, 692, 271, 1074, 1442, 448, 771, 675, 1203, 1361, 651, 777, 450, 1278, 1214, 874, 1270, 756, 581, 184, 1533, 108, 1598, 310, 183, 1326, 1076, 165, 806, 1496, 1459, 656, 48, 805, 717, 1196, 446, 680, 1416, 959, 1094, 472, 688, 775, 1053, 686, 889, 393, 127, 308, 1325, 40, 989, 1152, 1418, 1114, 725, 882, 734, 1299, 292, 1593, 1070, 528, 720, 583, 195, 367, 1057, 1587, 1197, 1276, 77, 103, 1521, 773, 507, 694, 1462, 322, 375, 633, 809, 335, 199, 1253, 770, 439, 13, 535, 90, 511, 1246, 859, 649, 1655, 611, 27, 152, 849, 1380, 590, 703, 97, 968, 1585, 370, 497, 92, 1207, 944, 248, 1182, 967, 1266, 1436, 496, 713, 1565, 1156, 1034, 976, 1100, 1083, 291, 457, 873, 159, 993, 124, 337, 1281, 341, 1362, 955, 294, 1341, 246, 587, 714, 299, 123, 9, 107, 1035, 492, 272, 471, 1478, 29, 26, 1499, 1538, 645, 1457, 1293, 1288, 1363, 1271, 1346, 1569, 957, 1465, 513, 870, 1541, 1566, 1262, 247, 1119, 402, 538, 389, 351, 936, 1078, 126, 1165, 1529, 55, 1242, 737, 1250, 595, 1088, 733, 897, 1506, 1010, 1080, 136, 1187, 1045, 1607, 316, 1564, 1013, 1488, 718, 1087, 630, 869, 1123, 321, 1676, 850, 1170, 245, 340, 176, 83, 1044, 509, 368, 1129, 1140, 1032, 1572, 880, 1511, 1150, 815, 268, 476, 827, 830, 1315, 1006, 1333, 1031, 1379, 969, 1263, 289, 1486, 305, 1023, 879, 1332, 111, 1316, 926, 197, 99, 1181, 354, 740, 80, 1364, 732, 1567, 284, 1089, 285, 615, 605, 847, 1050, 1160, 915, 379, 1477, 772, 1183, 1414, 1374, 121, 41, 1061, 46, 6, 749, 684, 334, 1206, 1411, 1609, 1610, 921, 848, 1626, 752, 719, 347, 975, 426, 187, 1306, 392, 1427, 385, 181, 935, 130, 1605, 1534, 320, 361, 1249, 201, 1435, 269, 954, 687, 863, 177, 846, 204, 1537, 887, 1273, 839, 1574, 1286, 685, 1222, 435, 101, 373, 1240, 470, 1376, 11, 1275, 866, 91, 1466, 329, 1412, 1550, 1211, 910, 1144, 1096, 578, 1468, 1580, 349, 172, 288, 883, 1093, 1665, 917, 1430, 664, 1469, 973, 904, 343, 1664, 961, 306, 1226, 898, 391, 1395, 515, 861, 1378, 1485, 761, 1526, 616, 1660, 1283, 189, 691, 377, 1216, 1410, 1065, 627, 986, 1255, 1021, 1238, 119, 366, 1015, 280, 642, 871, 949, 303, 1633, 1132, 421, 100, 1461, 1545, 348, 473, 930, 307, 174, 1036, 1390, 1267, 1628, 1638, 708, 881, 1279, 1682, 1308, 7, 1106, 84, 1644, 867, 1245, 1643, 1301, 1077, 1007, 474, 442, 816, 1304, 278, 1647, 1568, 178, 1081, 689, 240, 1046, 698, 885, 1016, 1241, 780, 1502, 632, 704, 711, 1455, 1377, 162, 1258, 896, 142, 747, 242, 801, 1264, 1296, 1476, 1025, 1599, 112, 1648, 743, 559, 1399, 589, 1060, 359, 1400, 296, 1235, 76, 757, 829, 923, 333, 342, 707, 1608, 1056, 851, 845, 644, 122, 1543, 1137, 797, 539, 352, 198, 427, 978, 1311, 1554, 262, 992, 1350, 1629, 890, 758, 671, 196, 1546, 258, 580, 795, 257, 1381, 1589, 455, 769, 1576, 1310, 1515, 948, 151, 1456, 900, 1674, 1340, 44, 1192, 345, 24, 256, 241, 1344, 1091, 505, 1331, 988, 971, 1218, 1356, 466, 1005, 1347, 104, 899, 1236, 168, 1184, 71, 1146, 411, 117, 619, 1620, 647, 488, 1252, 420, 1318, 1448, 191, 1365, 555, 267, 646, 1158, 1099, 266, 1606, 50, 1432, 1371, 939, 892, 1657, 1302, 802, 865, 1159, 1656, 1111, 970, 454, 1388, 1393, 1649, 552, 403, 1679, 1373, 1646, 1151, 1154, 1143, 820, 1348, 330, 1256, 573, 1097, 561, 840, 323, 1257, 114, 1259, 1047, 1558, 462, 1513, 1669, 1127, 1027, 336, 461, 746, 19, 1353, 1307, 1128, 249, 984, 1375, 1204, 1681, 283, 1642, 1487, 311, 662, 1668, 214, 1650, 1387, 1578, 312, 264, 1338, 918, 634, 753, 1048, 1501, 1313, 297, 326, 286, 1319, 109, 1570, 1002, 541, 1248, 1349, 1591, 1673, 990, 665, 1030, 1483, 1174, 659, 1463, 434, 1324, 1588, 315, 1582, 1547, 534, 1508, 1544, 1497, 422, 1118, 451, 640, 186, 251, 1386, 501, 701, 1590, 1198, 522, 290, 530, 916, 1090, 1482, 1105, 254, 1460, 586, 353, 188, 1175, 837, 1404, 1640, 670, 1492, 45, 726, 678, 1285, 1133, 637, 860, 1618, 919, 1102, 1168, 467, 263, 144, 1549, 1149, 295, 1402, 1247, 145, 1536, 1051, 1011, 132, 796, 477, 985, 1389, 893, 1161, 597, 1280, 1434, 679, 431, 1494, 568, 1491, 1653, 1527, 748, 1612, 69, 244, 1155, 344, 301, 1234, 960, 729, 8, 1122, 1498, 592, 563, 298, 193, 1251, 1085, 1419, 206, 1429, 243, 1420, 190, 1504, 1345, 362, 544, 1449, 182, 350, 1024, 1171, 1670, 903, 228, 170, 1525, 1009, 1215, 891, 600, 468, 510, 841, 32, 683, 1592, 1636, 811, 405, 1141, 1433, 1634, 1507, 759, 546, 1178, 894, 852, 817, 1532, 1300, 906, 690, 95, 1631, 1405, 660, 1594, 512, 1645, 1663, 98, 390, 677, 1559, 942, 1199, 1571, 1475, 1339, 914, 902, 1225, 666, 169, 814, 1265, 37, 1237, 1329, 1166, 61, 1243, 484, 1624, 1320, 216, 1613, 1583, 705, 157, 987, 502, 1018, 1019, 1514, 14, 838, 1431, 1556, 1601, 1360, 1666, 1062, 1352, 868, 909, 994, 878, 1600, 905, 876, 1384, 1086, 1535, 1130, 1372, 192, 1641, 281, 102, 1233, 1294, 1619, 1193, 813, 163, 1677, 750, 10, 1520, 999, 360, 853, 1415, 1342, 444, 1661, 1542, 533, 252, 1406, 1039, 617, 459, 1103, 567, 408, 1212, 113, 1575, 30, 938, 907, 1500, 314, 1385, 1563, 1586, 834, 1323, 799, 158, 424, 828, 410, 1493, 1553, 1142, 255, 760, 521, 514, 937, 180, 179, 18, 1292, 911, 1615, 1291, 1026, 1327, 653, 673, 1003, 532, 167, 1403, 1662, 1277, 89, 1022, 346, 762, 1200, 1398, 225, 313, 1623, 565, 217, 116, 784, 15, 1125, 433, 1038, 1229, 1548, 1428, 253, 1622, 59, 554, 1272, 570, 1033, 1531, 20, 1191, 1581, 211, 277, 1138, 641, 912, 309, 1359, 1228, 33, 1437, 413, 1330, 449, 72, 125, 927, 1473, 857, 901, 60, 572, 706, 304, 1322, 667, 884, 1107, 1274, 599, 895, 1635, 908, 1321, 643, 1671, 1290, 409, 1334, 115, 657, 1309, 1029, 941, 75, 185, 1561, 826, 1573, 575, 1366, 31, 388, 22, 1176, 1678, 779, 1516, 551, 751, 355, 1659, 1343, 576, 1358, 1109, 94, 722, 1201, 1630, 929, 1001, 1116, 913, 399, 1260, 282, 1335, 831, 1104, 1484, 1391, 996, 755, 1117, 1639, 128, 1579, 1667, 596, 1219, 93, 1397, 543, 1336, 824, 491, 1560, 1651, 1522, 754, 1439, 460, 1067, 1539, 768, 401, 741, 571, 400, 774, 143, 1012, 1108, 709, 767, 1602, 556, 702, 1555, 812, 16, 494, 1490, 1616, 803, 369, 1413, 585, 25, 612, 920, 1059, 1054, 738, 1261, 695, 1075, 1481, 798, 1317]\n",
      "checking recall: recall_val=0.5\n",
      "['0 : Three Colors: Red (1994) Drama',\n",
      " \"1 : Mary Shelley's Frankenstein (1994) Drama|Horror\",\n",
      " '2 : Romper Stomper (1992) Action|Drama',\n",
      " \"3 : Baby-Sitters Club, The (1995) Children's\",\n",
      " '4 : Suture (1993) Film-Noir|Thriller',\n",
      " '5 : Roommates (1995) Comedy|Drama',\n",
      " '6 : Outbreak (1995) Action|Drama|Thriller',\n",
      " '7 : Corrina, Corrina (1994) Comedy|Drama|Romance',\n",
      " '8 : Milk Money (1994) Comedy|Romance',\n",
      " '9 : Hunted, The (1995) Action',\n",
      " '10 : Perez Family, The (1995) Comedy|Romance',\n",
      " '11 : Walkabout (1971) Drama',\n",
      " '12 : Relative Fear (1994) Horror|Thriller',\n",
      " '13 : Tommy Boy (1995) Comedy',\n",
      " '14 : Dazed and Confused (1993) Comedy',\n",
      " '15 : Professional, The (1994) Crime|Drama|Romance|Thriller',\n",
      " '16 : Reality Bites (1994) Comedy|Drama',\n",
      " \"17 : Stephen King's The Langoliers (1995) Horror\",\n",
      " '18 : Natural Born Killers (1994) Action|Thriller',\n",
      " '19 : Paper, The (1994) Comedy|Drama']\n"
     ]
    }
   ],
   "source": [
    "summaries = {\n",
    "    \"Children\": \"Summary: animated toy adventures.\",  \n",
    "    \"Comedy\": \"Summary: A collection of comedy films spanning different eras and styles, featuring romantic entanglements, humorous misadventures, and comedic performances from an ensemble of actors.\",\n",
    "    }\n",
    "  \n",
    "get_preds(summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Editor built\n"
     ]
    }
   ],
   "source": [
    "args = get_args()\n",
    "editor = RobertaEditor(args).to(device)\n",
    "\n",
    "scorer = SteepHC(args,editor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = ''' Drama: Summary: A collection of drama films that explore themes of love, personal growth, and societal challenges. Each film tells a unique story with memorable characters and emotional depth.\n",
    "    Romance: Summary: A collection of romance films with diverse themes, including a Cuban refugee pretending to be a family, an erotic romantic thriller, a fantasy comedy about reliving the same day, a mistaken identity romantic comedy, a classic screwball comedy, a Shakespeare\n",
    "    Children: Summary: These children's movies are filled with fantasy, adventure, and heartwarming stories. From animated tales to magical adventures, these films are perfect for young viewers.\n",
    "    Comedy: Summary: A collection of comedy films spanning different eras and styles, featuring romantic entanglements, humorous misadventures, and comedic performances from an ensemble of actors.\n",
    "    Action: Summary: Action-packed films with a mix of genres including Western, medical disaster, buddy cop, science fiction, and horror anthology. The movies feature intense action sequences and diverse plots involving epic space battles, covert operations, and post-apocalyptic worlds.\n",
    "    Thriller: Summary: A collection of thrilling films with elements of psychological drama, legal suspense, science fiction, and horror. Featuring talented actors and directors, these movies keep audiences on the edge of their seats with intense storylines and unexpected twists.\n",
    "    Crime: Summary: A collection of crime films with various themes, including violence, drama, comedy, and thrillers, featuring notable directors and actors.\n",
    "    Adventure: Summary: Adventure movies filled with thrilling journeys, time travel, and epic battles. From exploring other planets through a mystical portal to going back in time or diving into a fantastical Arthurian world, these films take us on incredible escapades.\n",
    "    Sci-Fi: Summary: A collection of science fiction films that transport audiences to different worlds and challenge their perceptions of reality. These movies explore themes of adventure, disaster, and the power of technology in intriguing and unexpected ways.\n",
    "    Fantasy: Summary: Adventure comedies with a fantasy twist that provide an enjoyable escape from reality.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of words with ':' in the list: [0, 1, 29, 30, 71, 72, 97, 98, 122, 123, 160, 161, 199, 200, 220, 221, 260, 261, 297, 298]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Drama: Summary: A collection of drama films that explore themes of love, personal growth, and societal challenges. Each film tells a unique story with memorable characters and emotional depth.\n",
    "    Romance: Summary: A collection of romance films with diverse themes, including a Cuban refugee pretending to be a family, an erotic romantic thriller, a fantasy comedy about reliving the same day, a mistaken identity romantic comedy, a classic screwball comedy, a Shakespeare\n",
    "    Children: Summary: These children's movies are filled with fantasy, adventure, and heartwarming stories. From animated tales to magical adventures, these films are perfect for young viewers.\n",
    "    Comedy: Summary: A collection of comedy films spanning different eras and styles, featuring romantic entanglements, humorous misadventures, and comedic performances from an ensemble of actors.\n",
    "    Action: Summary: Action-packed films with a mix of genres including Western, medical disaster, buddy cop, science fiction, and horror anthology. The movies feature intense action sequences and diverse plots involving epic space battles, covert operations, and post-apocalyptic worlds.\n",
    "    Thriller: Summary: A collection of thrilling films with elements of psychological drama, legal suspense, science fiction, and horror. Featuring talented actors and directors, these movies keep audiences on the edge of their seats with intense storylines and unexpected twists.\n",
    "    Crime: Summary: A collection of crime films with various themes, including violence, drama, comedy, and thrillers, featuring notable directors and actors.\n",
    "    Adventure: Summary: Adventure movies filled with thrilling journeys, time travel, and epic battles. From exploring other planets through a mystical portal to going back in time or diving into a fantastical Arthurian world, these films take us on incredible escapades.\n",
    "    Sci-Fi: Summary: A collection of science fiction films that transport audiences to different worlds and challenge their perceptions of reality. These movies explore themes of adventure, disaster, and the power of technology in intriguing and unexpected ways.\n",
    "    Fantasy: Summary: Adventure comedies with a fantasy twist that provide an enjoyable escape from reality.\"\"\"\n",
    "\n",
    "# Split the text into words\n",
    "words = text.split()\n",
    "\n",
    "# Initialize a list to store the indices of words with ':'\n",
    "indices_of_words_with_colon = []\n",
    "\n",
    "# Iterate through the words to find the indices of words with ':'\n",
    "for index, word in enumerate(words):\n",
    "    if ':' in word:\n",
    "        # Add the index to the list\n",
    "        indices_of_words_with_colon.append(index)\n",
    "\n",
    "# Print the list of indices of words with ':'\n",
    "print(\"Indices of words with ':' in the list:\", indices_of_words_with_colon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_string_dict(string):\n",
    "        # Split the text into genre entries\n",
    "    genre_entries = text.strip().split('\\n')\n",
    "\n",
    "    # Initialize a dictionary to store the JSON data\n",
    "    genre_data = {}\n",
    "\n",
    "    # Iterate through the genre entries and populate the dictionary\n",
    "    for entry in genre_entries:\n",
    "        lines = entry.strip().split('\\n')\n",
    "        genre_name, genre_description = lines[0], lines[1:]\n",
    "        genre_data[genre_name] = \" \".join(genre_description)\n",
    "    return genre_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def style_ranker(text_new,original_text,movie_id =1073 ,original_index =16 ):\n",
    "    preds_new = get_preds(make_string_dict(text_new))\n",
    "    preds_old = get_preds(make_string_dict(original_text))\n",
    "    if 1073 in preds_new: \n",
    "        print(f\"{preds_new.index(1073) > preds_old.index(1073)=}\")\n",
    "        print(f\"{preds_old.index(1073)=}\")\n",
    "        print(f\"{preds_new.index(1073)=}\")\n",
    "        if preds_new.index(1073) > preds_old.index(1073):\n",
    "            return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =[data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Editor built\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import math\n",
    "import os\n",
    "import sys \n",
    "import logging\n",
    "sys.path.append('../')\n",
    "import torch.multiprocessing as mp\n",
    "import warnings\n",
    "import datetime\n",
    "from dateutil import tz\n",
    "import torch\n",
    "# from transformers import RobertaModel, RobertaTokenizer\n",
    "from StyleTransfer.scorer import *\n",
    "from StyleTransfer.editor import RobertaEditor\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "tzone = tz.gettz('')\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "args = get_args()\n",
    "\n",
    "editor = RobertaEditor(args).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/output/2023-11-10_10:48:51_yelp_seed=42_8_0-1.log\n"
     ]
    }
   ],
   "source": [
    "sahc = SteepHC(args, editor).to(device)\n",
    "of_dir = 'results/' + args.output_dir\n",
    "if not os.path.exists(of_dir):\n",
    "    os.makedirs(of_dir)\n",
    "\n",
    "if args.direction == '0-1': postfix = '0'\n",
    "else: postfix = '1'\n",
    "\n",
    "filename='../StyleTransfer/data/{}/test.{}'.format(args.dst,postfix)\n",
    "# with open(filename, 'r', encoding='utf8') as f:\n",
    "#     data = f.readlines()[:]\n",
    "\n",
    "bsz = args.bsz\n",
    "max_len=len(data[0])\n",
    "dst=args.dst\n",
    "num_batches = math.ceil(len(data) / float(bsz))\n",
    "timestamp = datetime.datetime.now().astimezone(tzone).strftime('%Y-%m-%d_%H:%M:%S')\n",
    "\n",
    "output_file =f'{timestamp}_{dst}_seed={str(args.seed)}_{str(args.style_weight)}_{args.direction}.txt'\n",
    "\n",
    "log_txt_path=os.path.join(of_dir, output_file.split('.txt')[0] + '.log')\n",
    "print(log_txt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2239"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 35.23 GB, other allocations: 1.07 GB, max allowed: 36.27 GB). Tried to allocate 84.55 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/emilianopenaloza/Git/LLM4Rec/notebooks/model_playground.ipynb Cell 19\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emilianopenaloza/Git/LLM4Rec/notebooks/model_playground.ipynb#X15sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m ref_new_batch_data\u001b[39m=\u001b[39mref_news[idx]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emilianopenaloza/Git/LLM4Rec/notebooks/model_playground.ipynb#X15sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m# Calculating the acceptance probability\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emilianopenaloza/Git/LLM4Rec/notebooks/model_playground.ipynb#X15sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m index, ref_old_score, ref_new_score, new_style_labels,_ \\\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/emilianopenaloza/Git/LLM4Rec/notebooks/model_playground.ipynb#X15sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     \u001b[39m=\u001b[39m sahc\u001b[39m.\u001b[39;49macceptance_prob(ref_new_batch_data, ref_olds, ref_oris, state_vec)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emilianopenaloza/Git/LLM4Rec/notebooks/model_playground.ipynb#X15sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m ref_hat \u001b[39m=\u001b[39m ref_new_batch_data[index]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emilianopenaloza/Git/LLM4Rec/notebooks/model_playground.ipynb#X15sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m new_style_label\u001b[39m=\u001b[39mnew_style_labels[index]\n",
      "File \u001b[0;32m~/Git/LLM4Rec/notebooks/../StyleTransfer/scorer.py:128\u001b[0m, in \u001b[0;36mSteepHC.acceptance_prob\u001b[0;34m(self, input_news, input_olds, ref_oris, state_vec, style_scorer)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39macceptance_prob\u001b[39m(\u001b[39mself\u001b[39m, input_news, input_olds,ref_oris,state_vec,style_scorer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 128\u001b[0m     ref_old_score, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscorer(input_olds, ref_oris, state_vec,style_scorer)\n\u001b[1;32m    129\u001b[0m     ref_new_scores, new_style_labels\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscorer(input_news,ref_oris,state_vec,style_scorer)\n\u001b[1;32m    131\u001b[0m     ref_new_score_index\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39margmax(ref_new_scores)\n",
      "File \u001b[0;32m~/Git/LLM4Rec/notebooks/../StyleTransfer/scorer.py:118\u001b[0m, in \u001b[0;36mSteepHC.scorer\u001b[0;34m(self, input_news, ref_oris, state_vec, style_scorer)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscorer\u001b[39m(\u001b[39mself\u001b[39m, input_news,ref_oris,state_vec\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,style_scorer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 118\u001b[0m     fluency_scores\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfluency_scorer(input_news) \u001b[39m# input-news, ref_oris-->[\"I like you\"]\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     style_scores,style_labels\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstyle_scorer(input_news) \u001b[39mif\u001b[39;00m style_scorer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m style_scorer(input_news) \u001b[39m# input-news, ref_oris-->[\"I like you\"]\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     sim_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msemantic_scorer(input_news, ref_oris, state_vec)\u001b[39m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/Git/LLM4Rec/notebooks/../StyleTransfer/scorer.py:71\u001b[0m, in \u001b[0;36mSteepHC.fluency_scorer\u001b[0;34m(self, ref_news)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39m# Ref: https://github.com/huggingface/transformers/issues/473\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 71\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(input_ids, labels\u001b[39m=\u001b[39;49mtarget_ids)\n\u001b[1;32m     73\u001b[0m     lm_logits\u001b[39m=\u001b[39moutputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m     74\u001b[0m     shift_logits \u001b[39m=\u001b[39m lm_logits[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, :input_ids\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\u001b[39m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/mambaforge/envs/torch_gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/torch_gpu/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:1109\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[39m# Flatten the tokens\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m     loss_fct \u001b[39m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m-> 1109\u001b[0m     loss \u001b[39m=\u001b[39m loss_fct(shift_logits\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, shift_logits\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)), shift_labels\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m   1111\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1112\u001b[0m     output \u001b[39m=\u001b[39m (lm_logits,) \u001b[39m+\u001b[39m transformer_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/mambaforge/envs/torch_gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/torch_gpu/lib/python3.9/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/mambaforge/envs/torch_gpu/lib/python3.9/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 35.23 GB, other allocations: 1.07 GB, max allowed: 36.27 GB). Tried to allocate 84.55 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "import random\n",
    "import ray\n",
    "from pprint import pprint as pp\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "logging.basicConfig(format='',\n",
    "                    filename=log_txt_path,\n",
    "                    filemode='w',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "word_pairs ={\"ca n't\": \"can not\", \"wo n't\": \"will not\"}\n",
    "logging.info(args)\n",
    "\n",
    "def print_es():\n",
    "    print(\"Early Stopping!\")\n",
    "    logging.info(\"Early Stopping!\")\n",
    "\n",
    "\n",
    "with open(of_dir + output_file, 'w', encoding='utf8') , torch.no_grad():\n",
    "    for i in range(num_batches):\n",
    "        batch_data = data[bsz * i:bsz * (i + 1)]\n",
    "\n",
    "        # print(f\"{batch_data=}\")\n",
    "\n",
    "        #preprocessing\n",
    "        ref_oris = []\n",
    "        for d in batch_data:\n",
    "            for k, v in word_pairs.items():\n",
    "                    d=d.strip().lower().replace(k, v)\n",
    "            ref_oris.append(d)\n",
    "\n",
    "        ref_olds=ref_oris.copy()\n",
    "        state_vec, _ = editor.state_vec(ref_olds)\n",
    "\n",
    "\n",
    "        break_flag = False\n",
    "        max_score=0\n",
    "        step_max_score_list=[0]\n",
    "        seq_len=[len(line.split()) for line in ref_olds]\n",
    "        max_seq_len=max(seq_len)\n",
    "\n",
    "        for step in range(args.max_steps):\n",
    "            #get the whole candidate list\n",
    "            sampled_indices = random.sample(range(max_len), 1)\n",
    "            input_tuples = [[ref_olds,[ops]*bsz,[positions]*bsz,bsz,max_len]\n",
    "                                                    for positions in sampled_indices if positions not in indices_of_words_with_colon for ops in [0,1,2]]\n",
    "\n",
    "\n",
    "            ref_news = [editor.edit(*inp)for inp in input_tuples[:1]]\n",
    "\n",
    "                                                    \n",
    "\n",
    "            for idx in range(len(ref_news)):\n",
    "\n",
    "                ref_new_batch_data=ref_news[idx]\n",
    "\n",
    "                # Calculating the acceptance probability\n",
    "\n",
    "                index, ref_old_score, ref_new_score, new_style_labels,_ \\\n",
    "                    = sahc.acceptance_prob(ref_new_batch_data, ref_olds, ref_oris, state_vec)\n",
    "\n",
    "                ref_hat = ref_new_batch_data[index]\n",
    "                new_style_label=new_style_labels[index]\n",
    "                \n",
    "                # Updating the maximum score and selected sentence\n",
    "                if ref_new_score>max_score and ref_new_score>ref_old_score:\n",
    "                    max_score=ref_new_score\n",
    "                    select_sent = ref_hat\n",
    "\n",
    "                # the style is changed!\n",
    "                if args.early_stop == True:\n",
    "                    if (args.direction == '0-1' and new_style_label == 1) or \\\n",
    "                            (args.direction == '1-0' and new_style_label == 0) :\n",
    "                        select_sent = ref_hat\n",
    "                        print_es()\n",
    "                        break_flag = True\n",
    "                        break\n",
    "            \n",
    "            # Checking if the current score is larger than previous max score\n",
    "            if max_score>step_max_score_list[step]: \n",
    "                print(\"hill climbing!\")\n",
    "                logging.info(\"hill climbing!\")\n",
    "                ref_olds = [select_sent]\n",
    "                step_max_score_list.append(max_score.item())\n",
    "            else:\n",
    "                print(\"don't climb, stop!\")\n",
    "                logging.info(\"don't climb, stop!\")\n",
    "                break_flag=True\n",
    "\n",
    "            if break_flag:\n",
    "                break\n",
    "\n",
    "        if break_flag:\n",
    "            select_sent = select_sent\n",
    "\n",
    "        logging.info('climb {} steps, the selected sentence is: {}'.format(step+1,select_sent))\n",
    "        print('climb {} steps, the selected sentence is: {}'.format(step+1,select_sent))\n",
    "\n",
    "\n",
    "        logging.info('\\n')\n",
    "     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
