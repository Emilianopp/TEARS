{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "df = pd.read_csv('//home/mila/e/emiliano.penaloza/LLM4REC/data_preprocessed/netflix/strong_generalization_set_.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 =  pd.read_csv('/home/mila/e/emiliano.penaloza/LLM4REC/data_preprocessed/netflix/test_tr.csv')\n",
    "d2 = pd.read_csv('/home/mila/e/emiliano.penaloza/LLM4REC/data_preprocessed/netflix/test_te.csv')\n",
    "d = pd.concat([d,d2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5700426718959304"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rating.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "644"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.groupby('uid').size().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1374"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('userId').size().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "        df = pd.read_csv('/home/mila/e/emiliano.penaloza/LLM4REC/data/goodbooks/genres.csv')\n",
    "        df.genres = df.genres.apply(ast.literal_eval)\n",
    "\n",
    "        mapping = {}\n",
    "        with open('/home/mila/e/emiliano.penaloza/LLM4REC/data_preprocessed/goodbooks/show2id.pkl','rb') as f :\n",
    "            item_id_map = pickle.load(f)\n",
    "        for index, row in df.iterrows():\n",
    "            if row['book_id'] in item_id_map:\n",
    "                    mapping[item_id_map[row['book_id']]] = row['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Amerika'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(mapping)=8093\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(mapping)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bures-Wasserstein Distances: tensor([0.7679, 2.1133])\n",
      "torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def bures_wasserstein_distance_vectorized(means1, covs1, means2, covs2):\n",
    "    # Compute squared L2 norm of differences in means\n",
    "    mean_diff = means1 - means2\n",
    "    mean_dist_squared = torch.sum(mean_diff ** 2, dim=1)  # Sum over columns to get a vector of shape (b,)\n",
    "\n",
    "    # Cholesky decomposition of covs1 in batch\n",
    "    sqrt_covs1 = torch.linalg.cholesky(covs1)\n",
    "\n",
    "    # Batch matrix product: sqrt_covs1 * covs2 * sqrt_covs1\n",
    "    # First part of the product: intermediate = sqrt_covs1 * covs2\n",
    "    intermediate = torch.bmm(sqrt_covs1, covs2)\n",
    "    # Second part of the product: product_matrix = intermediate * sqrt_covs1.transpose(1, 2)\n",
    "    product_matrix = torch.bmm(intermediate, sqrt_covs1.transpose(1, 2))\n",
    "\n",
    "    # Cholesky decomposition of the product_matrix in batch\n",
    "    sqrt_middle = torch.linalg.cholesky(product_matrix)\n",
    "\n",
    "    # Trace of sqrt_middle\n",
    "    trace_sqrt_middle = torch.diagonal(sqrt_middle, dim1=-2, dim2=-1).sum(-1)\n",
    "\n",
    "    # Trace of cov1 + cov2\n",
    "    trace_covs1 = torch.diagonal(covs1, dim1=-2, dim2=-1).sum(-1)\n",
    "    trace_covs2 = torch.diagonal(covs2, dim1=-2, dim2=-1).sum(-1)\n",
    "    trace_cov_sum = trace_covs1 + trace_covs2\n",
    "\n",
    "    # Total trace term\n",
    "    trace_term = trace_cov_sum - 2 * trace_sqrt_middle\n",
    "\n",
    "    # Total Bures-Wasserstein distance\n",
    "    distance = mean_dist_squared + trace_term\n",
    "    return distance\n",
    "\n",
    "# Example usage\n",
    "means1 = torch.tensor([[0.0, 0.0], [1.0, 1.0]], dtype=torch.float)\n",
    "covs1 = torch.tensor([[[1.0, 0.5], [0.5, 1.0]], [[1.0, 0.1], [0.1, 1.0]]], dtype=torch.float)\n",
    "\n",
    "means2 = torch.tensor([[0.5, 0.5], [2.0, 2.0]], dtype=torch.float)\n",
    "covs2 = torch.tensor([[[1.0, 0.0], [0.0, 1.0]], [[1.5, 0.0], [0.0, 1.5]]], dtype=torch.float)\n",
    "\n",
    "distances = bures_wasserstein_distance_vectorized(means1, covs1, means2, covs2)\n",
    "print(\"Bures-Wasserstein Distances:\", distances)\n",
    "print(covs1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/e/emiliano.penaloza/llm4rec/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/mila/e/emiliano.penaloza/llm4rec/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import os \n",
    "PATH = '/home/user/NEW_MODEL_CACHE/'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/home/mila/e/emiliano.penaloza/scratch/'\n",
    "os.environ['HF_HOME'] = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "os.environ['TORCH_HOME'] = '/home/mila/e/emiliano.penaloza/scratch/models'\n",
    "import torch.optim as optim\n",
    "from peft import get_peft_model\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time \n",
    "import torch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from model.MF import *\n",
    "from helper.dataloader import *\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from trainer.transformer_utilts import *\n",
    "from peft import LoraConfig, TaskType\n",
    "from transformers import T5Tokenizer\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import random \n",
    "import argparse\n",
    "from typing import List\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import torch.distributed as dist\n",
    "import numpy as np\n",
    "import json\n",
    "import os \n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from helper.eval_metrics import Recall_at_k_batch,NDCG_binary_at_k_batch,MRR_at_k\n",
    "import wandb\n",
    "from helper.dataloader import DataMatrix,MatrixDataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import DataLoader\n",
    "import sys \n",
    "\n",
    "from vae import data\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "from trainer.losses.loss import get_loss\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL NAME = /home/mila/e/emiliano.penaloza/scratch/saved_model/ml-1m/Transformer/best_model_lr_0.0001_embedding_module_microsoft_phi-2_epochs_3_l2_lambda_0.0001_lora_alpha_16_lora_r_16_bce_softmax__bias_False.pth\n",
      "args.model_log_name='Transformer_embedding_module_microsoft_phi-2_l2_lambda_0.0001_lora_r_16_scheduler_linear_decay_0.0001_0.1.csv'\n"
     ]
    }
   ],
   "source": [
    "args = parse_args(notebook=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp:\n",
      "- modeling_mistral_encoder.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 11.74it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.03s/it]\n",
      "Some weights of the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2 were not used when initializing MistralEncoderModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing MistralEncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MistralEncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/mila/e/emiliano.penaloza/llm4rec/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.mistral.base_model.model.embed_tokens.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.0.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.0.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.0.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.0.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.0.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.0.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.0.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.0.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.0.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.0.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.0.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.0.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.0.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.0.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.0.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.0.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.0.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.0.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.0.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.0.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.0.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.0.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.0.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.1.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.1.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.1.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.1.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.1.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.1.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.1.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.1.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.1.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.1.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.1.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.1.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.1.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.1.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.1.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.1.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.1.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.1.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.1.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.1.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.1.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.1.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.1.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.2.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.2.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.2.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.2.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.2.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.2.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.2.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.2.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.2.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.2.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.2.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.2.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.2.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.2.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.2.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.2.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.2.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.2.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.2.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.2.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.2.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.2.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.2.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.3.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.3.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.3.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.3.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.3.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.3.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.3.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.3.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.3.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.3.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.3.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.3.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.3.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.3.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.3.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.3.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.3.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.3.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.3.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.3.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.3.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.3.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.3.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.4.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.4.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.4.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.4.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.4.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.4.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.4.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.4.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.4.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.4.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.4.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.4.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.4.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.4.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.4.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.4.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.4.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.4.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.4.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.4.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.4.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.4.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.4.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.5.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.5.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.5.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.5.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.5.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.5.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.5.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.5.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.5.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.5.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.5.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.5.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.5.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.5.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.5.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.5.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.5.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.5.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.5.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.5.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.5.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.5.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.5.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.6.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.6.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.6.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.6.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.6.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.6.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.6.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.6.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.6.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.6.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.6.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.6.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.6.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.6.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.6.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.6.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.6.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.6.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.6.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.7.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.7.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.7.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.7.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.7.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.7.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.7.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.7.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.7.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.7.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.7.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.7.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.7.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.7.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.7.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.7.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.7.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.7.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.7.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.8.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.8.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.8.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.8.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.8.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.8.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.8.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.8.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.8.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.8.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.8.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.8.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.8.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.8.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.8.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.8.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.8.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.8.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.8.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.9.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.9.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.9.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.9.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.9.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.9.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.9.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.9.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.9.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.9.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.9.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.9.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.9.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.9.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.9.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.9.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.9.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.9.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.9.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.10.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.10.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.10.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.10.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.10.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.10.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.10.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.10.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.10.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.10.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.10.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.10.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.10.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.10.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.10.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.10.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.10.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.10.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.10.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.11.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.11.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.11.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.11.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.11.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.11.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.11.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.11.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.11.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.11.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.11.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.11.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.11.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.11.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.11.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.11.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.11.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.11.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.11.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.12.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.12.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.12.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.12.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.12.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.12.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.12.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.12.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.12.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.12.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.12.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.12.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.12.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.12.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.12.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.12.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.12.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.12.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.12.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.13.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.13.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.13.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.13.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.13.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.13.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.13.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.13.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.13.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.13.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.13.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.13.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.13.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.13.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.13.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.13.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.13.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.13.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.13.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.14.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.14.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.14.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.14.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.14.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.14.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.14.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.14.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.14.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.14.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.14.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.14.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.14.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.14.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.14.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.14.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.14.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.14.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.14.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.15.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.15.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.15.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.15.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.15.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.15.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.15.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.15.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.15.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.15.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.15.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.15.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.15.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.15.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.15.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.15.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.15.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.15.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.15.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.16.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.16.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.16.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.16.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.16.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.16.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.16.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.16.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.16.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.16.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.16.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.16.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.16.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.16.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.16.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.16.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.16.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.16.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.16.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.16.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.16.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.16.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.16.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.17.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.17.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.17.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.17.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.17.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.17.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.17.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.17.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.17.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.17.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.17.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.17.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.17.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.17.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.17.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.17.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.17.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.17.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.17.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.17.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.17.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.17.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.17.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.18.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.18.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.18.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.18.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.18.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.18.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.18.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.18.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.18.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.18.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.18.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.18.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.18.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.18.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.18.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.18.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.18.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.18.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.18.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.18.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.18.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.18.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.18.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.19.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.19.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.19.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.19.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.19.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.19.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.19.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.19.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.19.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.19.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.19.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.19.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.19.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.19.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.19.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.19.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.19.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.19.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.19.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.19.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.19.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.19.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.19.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.20.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.20.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.20.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.20.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.20.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.20.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.20.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.20.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.20.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.20.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.20.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.20.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.20.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.20.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.20.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.20.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.20.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.20.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.20.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.20.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.20.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.20.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.20.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.21.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.21.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.21.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.21.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.21.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.21.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.21.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.21.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.21.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.21.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.21.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.21.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.21.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.21.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.21.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.21.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.21.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.21.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.21.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.21.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.21.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.21.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.21.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.22.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.22.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.22.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.22.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.22.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.22.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.22.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.22.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.22.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.22.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.22.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.22.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.22.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.22.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.22.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.22.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.22.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.22.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.22.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.22.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.22.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.22.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.22.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.23.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.23.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.23.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.23.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.23.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.23.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.23.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.23.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.23.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.23.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.23.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.23.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.23.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.23.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.23.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.23.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.23.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.23.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.23.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.23.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.23.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.23.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.23.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.24.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.24.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.24.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.24.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.24.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.24.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.24.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.24.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.24.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.24.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.24.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.24.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.24.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.24.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.24.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.24.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.24.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.24.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.24.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.24.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.24.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.24.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.24.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.25.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.25.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.25.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.25.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.25.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.25.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.25.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.25.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.25.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.25.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.25.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.25.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.25.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.25.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.25.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.25.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.25.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.25.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.25.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.25.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.25.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.25.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.25.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.26.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.26.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.26.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.26.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.26.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.26.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.26.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.26.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.26.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.26.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.26.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.26.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.26.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.26.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.26.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.26.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.26.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.26.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.26.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.26.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.26.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.26.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.26.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.27.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.27.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.27.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.27.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.27.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.27.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.27.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.27.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.27.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.27.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.27.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.27.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.27.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.27.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.27.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.27.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.27.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.27.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.27.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.27.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.27.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.27.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.27.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.28.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.28.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.28.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.28.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.28.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.28.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.28.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.28.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.28.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.28.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.28.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.28.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.28.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.28.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.28.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.28.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.28.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.28.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.28.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.28.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.28.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.28.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.28.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.29.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.29.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.29.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.29.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.29.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.29.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.29.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.29.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.29.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.29.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.29.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.29.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.29.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.29.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.29.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.29.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.29.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.29.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.29.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.29.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.29.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.29.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.29.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.30.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.30.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.30.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.30.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.30.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.30.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.30.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.30.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.30.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.30.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.30.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.30.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.30.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.30.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.30.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.30.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.30.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.30.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.30.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.30.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.30.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.30.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.30.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.31.self_attn.q_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.31.self_attn.q_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.31.self_attn.q_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.31.self_attn.k_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.31.self_attn.k_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.31.self_attn.k_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.31.self_attn.v_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.31.self_attn.v_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.31.self_attn.v_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.31.self_attn.o_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.31.self_attn.o_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.31.self_attn.o_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.31.mlp.gate_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.31.mlp.gate_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.31.mlp.gate_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.31.mlp.up_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.31.mlp.up_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.31.mlp.up_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.31.mlp.down_proj.base_layer.weight torch.uint8\n",
      "base_model.model.mistral.base_model.model.layers.31.mlp.down_proj.lora_A.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.31.mlp.down_proj.lora_B.default.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.31.input_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.layers.31.post_attention_layernorm.weight torch.float32\n",
      "base_model.model.mistral.base_model.model.norm.weight torch.float32\n",
      "base_model.model.classifier.original_module.dense.weight torch.float32\n",
      "base_model.model.classifier.original_module.out_proj.weight torch.float32\n",
      "base_model.model.classifier.modules_to_save.default.dense.weight torch.float32\n",
      "base_model.model.classifier.modules_to_save.default.out_proj.weight torch.float32\n"
     ]
    }
   ],
   "source": [
    "args.embedding_module = \"McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp\"\n",
    "tokenizer = get_tokenizer(args)\n",
    "\n",
    "model,lora_config = get_model(args, tokenizer, 3522, 0, 1)\n",
    "model = get_peft_model(model, lora_config)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.AdamW(params, lr=args.lr,weight_decay=args.l2_lambda)\n",
    "\n",
    "model.to('cuda')\n",
    "#check dtype of weights \n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41005481984, 47758180352)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "torch.cuda.mem_get_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.bs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Prompt Length 262\n",
      "Number of Users is num_users=6016\n",
      "Number of Movies is num_movies=3522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "rank = 0 \n",
    "e = 0 \n",
    "scheduler = get_scheduler(optimizer, args)\n",
    "\n",
    "world_size = 1 \n",
    "prompts,rec_dataloader,augmented_dataloader,num_movies,val_dataloader,test_dataloader,val_data_tr,test_data_tr= load_data(args,tokenizer,rank,world_size)\n",
    "pbar = tqdm(range(args.epochs))\n",
    "\n",
    "\n",
    "loss_f = get_loss(args.loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.mem_get_info()=(41005481984, 47758180352)\n",
      "torch.cuda.mem_get_info()=(42083418112, 47758180352)\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.mem_get_info()\n",
    "print(f\"{torch.cuda.mem_get_info()=}\")\n",
    "torch.cuda.empty_cache() \n",
    "print(f\"{torch.cuda.mem_get_info()=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 44.48 GiB of which 183.88 MiB is free. Including non-PyTorch memory, this process has 44.29 GiB memory in use. Of the allocated memory 43.19 GiB is allocated by PyTorch, and 930.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m items[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(rank)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# print(f\"{torch.cuda.mem_get_info()=}\")\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     movie_emb \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# print(f\"{torch.cuda.mem_get_info()=}\")\u001b[39;00m\n\u001b[1;32m     19\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/peft/peft_model.py:937\u001b[0m, in \u001b[0;36mPeftModelForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;241m==\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mPOLY:\n\u001b[1;32m    936\u001b[0m             kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task_ids\n\u001b[0;32m--> 937\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    948\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    950\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:161\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM4REC/model/MF.py:756\u001b[0m, in \u001b[0;36mMistralClassifier.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, labels_tr, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    743\u001b[0m             input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    744\u001b[0m             attention_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    752\u001b[0m             output_hidden_states: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    753\u001b[0m             return_dict: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,):\n\u001b[0;32m--> 756\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    758\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier_forward(hidden_states)\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [logits]\n",
      "File \u001b[0;32m~/LLM4REC/model/MF.py:777\u001b[0m, in \u001b[0;36mMistralClassifier.llm_forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllm_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    767\u001b[0m             input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    768\u001b[0m             attention_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    775\u001b[0m             output_hidden_states: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    776\u001b[0m             return_dict: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,):\n\u001b[0;32m--> 777\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmistral\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    778\u001b[0m     features \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_ids, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: attention_mask}\n\u001b[1;32m    779\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_pooling(features,hidden_states)\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/peft/peft_model.py:563\u001b[0m, in \u001b[0;36mPeftModel.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    562\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/scratch/models/modules/transformers_modules/McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp/0e7b5e42198621ccf374fd21d6e0050d73d38bdf/modeling_mistral_encoder.py:177\u001b[0m, in \u001b[0;36mMistralEncoderModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    167\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    168\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    169\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m         use_cache,\n\u001b[1;32m    175\u001b[0m     )\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:770\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    768\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    769\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 770\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    773\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:179\u001b[0m, in \u001b[0;36mMistralMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:452\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_layer(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# As per Tim Dettmers, for 4bit, we need to defensively clone here.\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# The reason is that in some cases, an error can occur that backprop\u001b[39;00m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;66;03m# does not work on a manipulated view. This issue may be solved with\u001b[39;00m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;66;03m# newer PyTorch versions but this would need extensive testing to be\u001b[39;00m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;66;03m# sure.\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mclone()\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:468\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    465\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    467\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 468\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:579\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul4Bit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:509\u001b[0m, in \u001b[0;36mMatMul4Bit.forward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mempty(A\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m B_shape[:\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# 1. Dequantize\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# 2. MatmulnN\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlinear(A, \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdequantize_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(A\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mt(), bias)\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# 3. Save state\u001b[39;00m\n\u001b[1;32m    512\u001b[0m ctx\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m quant_state\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/bitsandbytes/functional.py:1349\u001b[0m, in \u001b[0;36mdequantize_4bit\u001b[0;34m(A, quant_state, absmax, out, blocksize, quant_type)\u001b[0m\n\u001b[1;32m   1346\u001b[0m         absmax \u001b[38;5;241m=\u001b[39m absmax\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1349\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1351\u001b[0m n \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mnumel()\n\u001b[1;32m   1353\u001b[0m device \u001b[38;5;241m=\u001b[39m pre_call(A\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 44.48 GiB of which 183.88 MiB is free. Including non-PyTorch memory, this process has 44.29 GiB memory in use. Of the allocated memory 43.19 GiB is allocated by PyTorch, and 930.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "#CLEAR CUDA CACHE   \n",
    "torch.cuda.empty_cache()\n",
    "start_time = time.time()\n",
    "\n",
    "# FILEPATH: /home/mila/e/emiliano.penaloza/LLM4REC/testing.ipynb\n",
    "\n",
    "for b, (items) in enumerate(rec_dataloader):\n",
    "    model.train()\n",
    "    labels = items['labels'].to(rank)\n",
    "\n",
    "    input_ids = items['input_ids'].to(rank)\n",
    "    attention_mask = items['attention_mask'].to(rank)\n",
    "\n",
    "    # print(f\"{torch.cuda.mem_get_info()=}\")\n",
    "\n",
    "    movie_emb = model(input_ids=input_ids.to(rank), attention_mask=attention_mask.to(rank))\n",
    "    # print(f\"{torch.cuda.mem_get_info()=}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 1025.0282628536224 seconds\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 44.48 GiB of which 175.88 MiB is free. Including non-PyTorch memory, this process has 44.30 GiB memory in use. Of the allocated memory 41.45 GiB is allocated by PyTorch, and 2.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m items[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(rank)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# print(f\"{torch.cuda.mem_get_info()=}\")\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     movie_emb \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# print(f\"{torch.cuda.mem_get_info()=}\")\u001b[39;00m\n\u001b[1;32m     40\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/peft/peft_model.py:937\u001b[0m, in \u001b[0;36mPeftModelForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;241m==\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mPOLY:\n\u001b[1;32m    936\u001b[0m             kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task_ids\n\u001b[0;32m--> 937\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    948\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    950\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:161\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM4REC/model/MF.py:756\u001b[0m, in \u001b[0;36mMistralClassifier.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, labels_tr, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    743\u001b[0m             input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    744\u001b[0m             attention_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    752\u001b[0m             output_hidden_states: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    753\u001b[0m             return_dict: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,):\n\u001b[0;32m--> 756\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    758\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier_forward(hidden_states)\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [logits]\n",
      "File \u001b[0;32m~/LLM4REC/model/MF.py:777\u001b[0m, in \u001b[0;36mMistralClassifier.llm_forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllm_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    767\u001b[0m             input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    768\u001b[0m             attention_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    775\u001b[0m             output_hidden_states: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    776\u001b[0m             return_dict: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,):\n\u001b[0;32m--> 777\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmistral\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    778\u001b[0m     features \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_ids, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: attention_mask}\n\u001b[1;32m    779\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_pooling(features,hidden_states)\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/peft/peft_model.py:563\u001b[0m, in \u001b[0;36mPeftModel.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    562\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/scratch/models/modules/transformers_modules/McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp/0e7b5e42198621ccf374fd21d6e0050d73d38bdf/modeling_mistral_encoder.py:177\u001b[0m, in \u001b[0;36mMistralEncoderModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    167\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    168\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    169\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m         use_cache,\n\u001b[1;32m    175\u001b[0m     )\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:770\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    768\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    769\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 770\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    773\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:179\u001b[0m, in \u001b[0;36mMistralMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:452\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_layer(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# As per Tim Dettmers, for 4bit, we need to defensively clone here.\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# The reason is that in some cases, an error can occur that backprop\u001b[39;00m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;66;03m# does not work on a manipulated view. This issue may be solved with\u001b[39;00m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;66;03m# newer PyTorch versions but this would need extensive testing to be\u001b[39;00m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;66;03m# sure.\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mclone()\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:468\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    465\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    467\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 468\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:579\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul4Bit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:509\u001b[0m, in \u001b[0;36mMatMul4Bit.forward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mempty(A\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m B_shape[:\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# 1. Dequantize\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# 2. MatmulnN\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlinear(A, \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdequantize_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(A\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mt(), bias)\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# 3. Save state\u001b[39;00m\n\u001b[1;32m    512\u001b[0m ctx\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m quant_state\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/bitsandbytes/functional.py:1349\u001b[0m, in \u001b[0;36mdequantize_4bit\u001b[0;34m(A, quant_state, absmax, out, blocksize, quant_type)\u001b[0m\n\u001b[1;32m   1346\u001b[0m         absmax \u001b[38;5;241m=\u001b[39m absmax\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1349\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1351\u001b[0m n \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mnumel()\n\u001b[1;32m   1353\u001b[0m device \u001b[38;5;241m=\u001b[39m pre_call(A\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 44.48 GiB of which 175.88 MiB is free. Including non-PyTorch memory, this process has 44.30 GiB memory in use. Of the allocated memory 41.45 GiB is allocated by PyTorch, and 2.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# FILEPATH: /home/mila/e/emiliano.penaloza/LLM4REC/testing.ipynb\n",
    "with torch.autocast(enabled=True, dtype=torch.float16, device_type=\"cuda\"):\n",
    "    for b, (items,augmented_items) in enumerate(zip(rec_dataloader,augmented_dataloader)):\n",
    "        model.train()\n",
    "        labels = items['labels'].to(rank)\n",
    "        input_ids = items['input_ids'].to(rank)\n",
    "        attention_mask = items['attention_mask'].to(rank)\n",
    "\n",
    "        # print(f\"{torch.cuda.mem_get_info()=}\")\n",
    "\n",
    "        movie_emb = model(input_ids=input_ids.to(rank), attention_mask=attention_mask.to(rank))\n",
    "        # print(f\"{torch.cuda.mem_get_info()=}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.48 GiB of which 61.88 MiB is free. Including non-PyTorch memory, this process has 44.41 GiB memory in use. Of the allocated memory 41.76 GiB is allocated by PyTorch, and 2.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m items[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(rank)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# print(f\"{torch.cuda.mem_get_info()=}\")\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     movie_emb \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# print(f\"{torch.cuda.mem_get_info()=}\")\u001b[39;00m\n\u001b[1;32m     18\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/peft/peft_model.py:937\u001b[0m, in \u001b[0;36mPeftModelForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;241m==\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mPOLY:\n\u001b[1;32m    936\u001b[0m             kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task_ids\n\u001b[0;32m--> 937\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    948\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    950\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:161\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LLM4REC/model/MF.py:756\u001b[0m, in \u001b[0;36mMistralClassifier.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, labels_tr, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    743\u001b[0m             input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    744\u001b[0m             attention_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    752\u001b[0m             output_hidden_states: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    753\u001b[0m             return_dict: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,):\n\u001b[0;32m--> 756\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    758\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier_forward(hidden_states)\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [logits]\n",
      "File \u001b[0;32m~/LLM4REC/model/MF.py:777\u001b[0m, in \u001b[0;36mMistralClassifier.llm_forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllm_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    767\u001b[0m             input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    768\u001b[0m             attention_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    775\u001b[0m             output_hidden_states: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    776\u001b[0m             return_dict: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,):\n\u001b[0;32m--> 777\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmistral\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    778\u001b[0m     features \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_ids, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: attention_mask}\n\u001b[1;32m    779\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_pooling(features,hidden_states)\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/peft/peft_model.py:563\u001b[0m, in \u001b[0;36mPeftModel.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    562\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/scratch/models/modules/transformers_modules/McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp/0e7b5e42198621ccf374fd21d6e0050d73d38bdf/modeling_mistral_encoder.py:177\u001b[0m, in \u001b[0;36mMistralEncoderModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    167\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    168\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    169\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m         use_cache,\n\u001b[1;32m    175\u001b[0m     )\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:757\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    754\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    756\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    765\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    767\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:653\u001b[0m, in \u001b[0;36mMistralSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m    643\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    644\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    648\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    649\u001b[0m     )\n\u001b[1;32m    651\u001b[0m bsz, q_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m--> 653\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    654\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[1;32m    655\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:452\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_layer(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# As per Tim Dettmers, for 4bit, we need to defensively clone here.\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# The reason is that in some cases, an error can occur that backprop\u001b[39;00m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;66;03m# does not work on a manipulated view. This issue may be solved with\u001b[39;00m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;66;03m# newer PyTorch versions but this would need extensive testing to be\u001b[39;00m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;66;03m# sure.\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mclone()\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:468\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    465\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    467\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 468\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:579\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul4Bit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:509\u001b[0m, in \u001b[0;36mMatMul4Bit.forward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mempty(A\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m B_shape[:\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# 1. Dequantize\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# 2. MatmulnN\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlinear(A, \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdequantize_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(A\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mt(), bias)\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# 3. Save state\u001b[39;00m\n\u001b[1;32m    512\u001b[0m ctx\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m quant_state\n",
      "File \u001b[0;32m~/llm4rec/lib/python3.10/site-packages/bitsandbytes/functional.py:1349\u001b[0m, in \u001b[0;36mdequantize_4bit\u001b[0;34m(A, quant_state, absmax, out, blocksize, quant_type)\u001b[0m\n\u001b[1;32m   1346\u001b[0m         absmax \u001b[38;5;241m=\u001b[39m absmax\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1349\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1351\u001b[0m n \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mnumel()\n\u001b[1;32m   1353\u001b[0m device \u001b[38;5;241m=\u001b[39m pre_call(A\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 44.48 GiB of which 61.88 MiB is free. Including non-PyTorch memory, this process has 44.41 GiB memory in use. Of the allocated memory 41.76 GiB is allocated by PyTorch, and 2.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_bf16_supported()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory with quantitization \n",
    "(41005481984, 47758180352)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm4rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
