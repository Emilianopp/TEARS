from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM, TextGenerationPipeline
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from transformers import LlamaTokenizer, LlamaForCausalLM
import transformers
import torch
import re
from argparse import ArgumentParser

access_token = "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"


def parse_args():  # Parse command line arguments
    parser = ArgumentParser(description="LLM4RecSys")
    parser.add_argument(
        "--model_name", default='llama2', type=str, help="model_type"
    )
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print("device:", device)

    model_dir = '/home/haolun/projects/ctb-lcharlin/haolun/saved_LLM'
    model_name = args.model_name

    if 't5' in model_name:
        tokenizer = AutoTokenizer.from_pretrained("google/{}".format(model_name), cache_dir=model_dir)
        model = AutoModelForSeq2SeqLM.from_pretrained("google/{}".format(model_name), cache_dir=model_dir).to(device)
    elif model_name == 'gpt2':
        tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
        model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)
    elif model_name == "falcon-7b-instruct":
        tokenizer = AutoTokenizer.from_pretrained("tiiuae/{}".format(model_name),
                                                  cache_dir=model_dir,
                                                  trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained("tiiuae/{}".format(model_name),
                                                     cache_dir=model_dir,
                                                     trust_remote_code=True).to(device)
    elif model_name == 'llama2':
        tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf",
                                                  cache_dir=model_dir,
                                                  trust_remote_code=True, use_auth_token=access_token)
        model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf",
                                                     cache_dir=model_dir,
                                                     trust_remote_code=True, use_auth_token=access_token).to(device)

    prompt = "Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:"
    input_string = prompt
    input_string = re.sub(' +', ' ', input_string)

    # Encode the input string
    input_ids = tokenizer.encode(input_string, return_tensors='pt', truncation=True, max_length=1024).to(device)
    print("input_ids:", len(input_ids[0]))
    print(f"Max token ID in input: {input_ids.max()}")

    max_length = 200
    outputs = model.generate(input_ids,
                             do_sample=True,  # Set to True to implement top-p sampling
                             max_length=max_length,
                             top_p=0.95,  # Adjust as needed
                             temperature=0.7,
                             num_return_sequences=5)

    # Decode only the new tokens that were generated by the model after the input tokens
    if model_name in ['gpt2', 'llama-3b', 'llama-7b', "falcon-7b-instruct", "llama2"]:
        decoded_outputs = tokenizer.batch_decode(outputs[:, input_ids.size(1):], skip_special_tokens=True)
    else:
        decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)

    # Loop over the decoded outputs and print them
    for i, decoded_output in enumerate(decoded_outputs):
        decoded_output = decoded_output.replace('\n', ', ')
        print(f"\n Output:\n{decoded_output}")

    # text_generator = TextGenerationPipeline(model=model, tokenizer=tokenizer, device='cpu')
    #
    # outputs = text_generator(input_string,
    #                          max_length=200,
    #                          do_sample=True,
    #                          # top_k=10,
    #                          top_p=0.95,
    #                          temperature=0.7,
    #                          num_return_sequences=1,
    #                          eos_token_id=tokenizer.eos_token_id)

    # model = "tiiuae/falcon-7b-instruct"
    # tokenizer = AutoTokenizer.from_pretrained(model)
    #
    # pipeline = transformers.pipeline(
    #     "text-generation",
    #     model=model,
    #     tokenizer=tokenizer,
    #     torch_dtype=torch.bfloat16,
    #     trust_remote_code=True,
    #     device_map="auto",
    # )
    #
    # sequences = pipeline(prompt,
    #                      max_length=200,
    #                      do_sample=True,
    #                      top_p=0.95,
    #                      num_return_sequences=5,
    #                      eos_token_id=tokenizer.eos_token_id,
    #                      )
    # for seq in sequences:
    #     print(f"Result: {seq['generated_text']}")
