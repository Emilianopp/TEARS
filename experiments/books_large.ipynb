{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/e/emiliano.penaloza/llm4rec/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/mila/e/emiliano.penaloza/llm4rec/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:158: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_path='./data_preprocessed/books/'\n",
      "Max Prompt Length 260\n",
      "Number of Users is num_users=3534\n",
      "Number of Movies is num_movies=14871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of sentenceT5Classification were not initialized from the model checkpoint at t5-large and are newly initialized: ['classification_head.out_proj.weight', 'classification_head.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): sentenceT5Classification(\n",
       "      (transformer): T5EncoderModel(\n",
       "        (shared): Embedding(32128, 1024)\n",
       "        (encoder): T5Stack(\n",
       "          (embed_tokens): Embedding(32128, 1024)\n",
       "          (block): ModuleList(\n",
       "            (0): T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                    (v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                    (relative_attention_bias): Embedding(32, 16)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseActDense(\n",
       "                    (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                    (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): ReLU()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1-23): 23 x T5Block(\n",
       "              (layer): ModuleList(\n",
       "                (0): T5LayerSelfAttention(\n",
       "                  (SelfAttention): T5Attention(\n",
       "                    (q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                    (v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1024, out_features=32, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (1): T5LayerFF(\n",
       "                  (DenseReluDense): T5DenseActDense(\n",
       "                    (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                    (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (act): ReLU()\n",
       "                  )\n",
       "                  (layer_norm): T5LayerNorm()\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): T5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (classification_head): ModulesToSaveWrapper(\n",
       "        (original_module): T5ClassificationHead(\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=14871, bias=False)\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): T5ClassificationHead(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (out_proj): Linear(in_features=1024, out_features=14871, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "\n",
    "import sys \n",
    "sys.path.append('../')\n",
    "import time\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from trainer.transformer_utilts import *\n",
    "from helper.dataloader import * \n",
    "import pickle\n",
    "from peft import get_peft_model\n",
    "from model.MF import sentenceT5Classification\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from peft import LoraConfig, TaskType\n",
    "import logging\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "logging.basicConfig(filename=f'./logs/{time.time()}.log',level=logging.INFO)\n",
    "logging.info(f'Experiment started at {time.time()}')\n",
    "\n",
    "\n",
    "def parse_args(notebook = False):  # Parse command line arguments\n",
    "    parser = argparse.ArgumentParser(description=\"LLM4RecSys\")\n",
    "    parser.add_argument(\"--data_name\", default='ml-1m', type=str)\n",
    "    parser.add_argument(\"--direction\", default='down', type=str)\n",
    "    parser.add_argument(\"--bs\", default=32, type=int)\n",
    "    parser.add_argument(\"--target_index\", default=49, type=int)\n",
    "    parser.add_argument(\"--num_samples\", default=250, type=int)\n",
    "    parser.add_argument(\"--max_l\", default=260, type=int)\n",
    "    parser.add_argument(\"--debug_prompts\", default=False, type=bool)\n",
    "    parser.add_argument(\"--metadata_path\",default='./data_preprocessed/books/train.csv',type=str)\n",
    "    args = parser.parse_args() if not notebook else parser.parse_args(args=[])\n",
    "    args.scratch = '/home/mila/e/emiliano.penaloza/scratch'\n",
    "    return args\n",
    "args = parse_args(True)\n",
    "args.data_name = 'books'\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "max_l = args.max_l\n",
    "key = os.getenv(\"OPEN-AI-SECRET\")\n",
    "openai.api_key = key\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-large\")\n",
    "prompts,rec_dataloader,num_movies,val_dataloader,test_dataloader,_,_= load_data(args,tokenizer,0,1)\n",
    "\n",
    "# Load model and mapping dicts\n",
    "path = f'{args.scratch}/saved_model/{args.data_name}/t5_classification_embedding_module_t5_classification_l2_lambda_0.0_lora_r_32_scheduler_cosine_warmup_0.001_0.2.csv.pt'\n",
    "model = sentenceT5Classification.from_pretrained('t5-large', num_labels=num_movies)\n",
    "\n",
    "lora_config = LoraConfig(task_type=TaskType.SEQ_CLS, r=32, lora_alpha=16, lora_dropout=0,\n",
    "                            target_modules=[\"q\", \"v\"],\n",
    "                            modules_to_save=['classification_head'])\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.load_state_dict(torch.load(path ,map_location=torch.device('cpu')))\n",
    "model.to(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f'./data_preprocessed/{args.data_name}/show2id.pkl','rb') as f:\n",
    "    movie_id_map = pickle.load(f)\n",
    "    movie_id_map = {v:k for k,v in movie_id_map.items()}\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "item_id_to_title = map_id_to_title( args.metadata_path,data = 'books')\n",
    "item_id_to_genre = map_id_to_genre(args.metadata_path,data = 'books')\n",
    "item_title_to_genre = {item_title: item_id_to_genre[item_id] for item_id, item_title in item_id_to_title.items()}\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_prompt(tokenizer,prompt,max_l):\n",
    "    encodings = tokenizer([prompt],padding=True, truncation=True,max_length=max_l,\n",
    "                          return_tensors='pt')\n",
    "    return encodings\n",
    "\n",
    "\n",
    "def genrewise_ndcg(genre_movies, genre, min_k=0, max_k=None):\n",
    "    relevance = [1 if genre in g   else 0 for g in genre_movies[min_k:max_k]]\n",
    "    dcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(relevance))\n",
    "    idcg = sum(1 / np.log2(i + 2) for i in range(len(relevance)))\n",
    "    ndcg = dcg / idcg if idcg > 0 else 0\n",
    "    \n",
    "    return ndcg\n",
    "\n",
    "# gets diffs in recommendations between two summaries\n",
    "def get_difs(s1,s2,device = 0,genre = 'Action' ,max_k = 20 ):\n",
    "\n",
    "    s_1_encodings = tokenize_prompt(tokenizer,s1,max_l)\n",
    "    s_2_encodings = tokenize_prompt(tokenizer,s2,max_l)\n",
    "    for k in s_1_encodings:\n",
    "        s_1_encodings[k] = s_1_encodings[k].to(device)\n",
    "    for k in s_2_encodings:\n",
    "        s_2_encodings[k] = s_2_encodings[k].to(device)\n",
    "    out_s1 = model(**s_1_encodings)\n",
    "    out_s2 = model(**s_2_encodings)\n",
    "    l = torch.topk(torch.softmax(out_s1.logits,dim = 1),1000)\n",
    "    indices_1 = l.indices[0]\n",
    "    movie_titles_1 = [item_id_to_title[item_id_map[index_1.item()]] for index_1 in indices_1] if args.data_name == 'ml-1m' else [item_id_to_title[index_1.item()] for index_1 in indices_1]\n",
    "    l2 = torch.topk(torch.softmax(out_s2.logits,dim = 1),1000)\n",
    "    indices_2 = l2.indices[0]\n",
    "    movie_titles_2 = [item_id_to_title[item_id_map[index_2.item()]] for index_2 in indices_2] if args.data_name == 'ml-1m' else [item_id_to_title[index_2.item()] for index_2 in indices_2]\n",
    "    genre_movies_1 = [item_title_to_genre[movie_title] for movie_title in movie_titles_1]\n",
    "    genre_movies_2 = [item_title_to_genre[movie_title] for movie_title in movie_titles_2]\n",
    "    ndcg_1 = genrewise_ndcg(genre_movies_1,genre,min_k = 0,max_k = max_k)\n",
    "    ndcg_2 = genrewise_ndcg(genre_movies_2,genre,min_k = 0,max_k = max_k)\n",
    "    delta = ndcg_1 - ndcg_2    \n",
    "\n",
    "    rankings1 = [(movie_title,genre) for movie_title,genre in zip(movie_titles_1,genre_movies_1)]\n",
    "    rankings2 = [(movie_title,genre) for movie_title,genre in zip(movie_titles_2,genre_movies_2)]\n",
    "\n",
    "\n",
    "\n",
    "    return delta,rankings1,rankings2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(sum([v.split(\"|\") for v in item_id_to_genre.values()],[]))\n",
    "#keep counts if above 200 \n",
    "counts = {k:v for k,v in counts.items() if v > 100}\n",
    "genre_set = list(counts.keys())\n",
    "genre_set = ', '.join(genre_set)\n",
    "k_list= list(prompts.keys())\n",
    "k_sample = random.sample(k_list,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "average ndcgs up -0.263729111333051 average ndcg down 0.22492106360262382: 100%|██████████| 500/500 [1:41:50<00:00, 12.22s/it]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(2024)\n",
    "# Prompt GPT to change summaries \n",
    "model.eval()\n",
    "moved_up = []\n",
    "moved_down = []\n",
    "average_medians_down = []\n",
    "average_medians_up = []\n",
    "move_up_genres =[]\n",
    "move_down_genres = []\n",
    "outputs = []\n",
    "keys = []\n",
    "deltas_ndcg_up = []\n",
    "deltas_ndcg_down = []\n",
    "deltas_ndcg_up = []\n",
    "deltas_ndcg_down = []\n",
    "for k in (pbar:=tqdm(k_sample)): \n",
    "    s = prompts[k]\n",
    "        \n",
    "    prompt = \\\n",
    "                f\"\"\"\n",
    "                You are a professional editor please identify the users preferred genres from the following:\n",
    "                {genre_set}\n",
    "                \"\"\"\n",
    "    user_prompt =\\\n",
    "                f\"\"\"\n",
    "                Please identify the users most favorite genre from the following summary and the least favorite genre: \n",
    "                in the format Favorite: [genre]\\n Least Favorite: [genre]\n",
    "                {s}.\n",
    "                \"\"\"\n",
    "                \n",
    "    msg = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": prompt\n",
    "\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt\n",
    "                }\n",
    "            ]\n",
    "\n",
    "    genres = openai.ChatCompletion.create(\n",
    "                    model='gpt-4-1106-preview',\n",
    "                    messages=msg,\n",
    "                    max_tokens=300,\n",
    "                    temperature=0.001,\n",
    "                    seed=2024,\n",
    "                )['choices'][0]['message']['content']\n",
    "    lines = genres.split('\\n')\n",
    "    favorite_genre = lines[0].split(': ')[1]\n",
    "    logging.info(f\"{favorite_genre=}\")\n",
    "    least_favorite_genre = lines[1].split(': ')[1]\n",
    "    logging.info(f\"{least_favorite_genre=}\")\n",
    "    \n",
    "    msg = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": prompt\n",
    "\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt\n",
    "                },\n",
    "                {'role': 'assistant',\n",
    "                'content': genres},\n",
    "                {'role':'user',\n",
    "                'content':\n",
    "                f'Now using this setup write the a new summary in the same style that reflects that {favorite_genre} is your least favorite\\\n",
    "                    and {least_favorite_genre} is your favorite only output the full summary keep the format and length the same' }\n",
    "            ]\n",
    "\n",
    "    gpt_output = openai.ChatCompletion.create(\n",
    "                    model='gpt-4-1106-preview',\n",
    "                    messages=msg,\n",
    "                    max_tokens=300,\n",
    "                    temperature=0.0000001,\n",
    "                    seed=2024,\n",
    "                )['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "    delta_down,rankings1_down,rankings2_down = get_difs(s,gpt_output,genre = favorite_genre)\n",
    "    delta_up,rankings1_up,rankings2_up = get_difs(s,gpt_output,genre = least_favorite_genre)\n",
    "    move_down_genres.append(favorite_genre)\n",
    "    move_up_genres.append(least_favorite_genre)\n",
    "    outputs.append(gpt_output)  \n",
    "    keys.append(k)\n",
    "    deltas_ndcg_down.append(delta_down)\n",
    "    deltas_ndcg_up.append(delta_up)\n",
    "\n",
    "    \n",
    "    pbar.set_description(f\"average ndcgs up {np.mean(deltas_ndcg_up)} average ndcg down {np.mean(deltas_ndcg_down)}\")    \n",
    "    logging.info(f\"average ndcgs up {np.mean(deltas_ndcg_up)} average ndcg down {np.mean(deltas_ndcg_down)}\")\n",
    "    \n",
    "\n",
    "data = {\n",
    "    'move_down_genres': move_down_genres,\n",
    "    'move_up_genres': move_up_genres,\n",
    "    'outputs': outputs,\n",
    "    'keys': keys,\n",
    "    'deltas_ndcg_down': deltas_ndcg_down,\n",
    "    'deltas_ndcg_up': deltas_ndcg_up,\n",
    "}\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(f'./results/{args.data_name}/gpt4_results_large_{args.direction}.csv')\n",
    "logging.info(f'./results/{args.data_name}/gpt4_results_large_{args.direction}.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm4rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
