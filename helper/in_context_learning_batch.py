import os
import json
import time
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from transformers import LlamaTokenizer, LlamaForCausalLM
from transformers import pipeline
import torch
import re
import openai
from tenacity import retry, wait_exponential, stop_after_attempt
from tqdm import tqdm

access_token = "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"
# global_path = '/home/haolun/projects/ctb-lcharlin/haolun/LLM4Rec_User_Summary'
global_path = '/Users/haolunwu/Documents/GitHub/LLM4Rec_User_Summary'


def load_tokenizer_model(model_name, device):
    model_dir = '/home/haolun/projects/ctb-lcharlin/haolun/saved_LLM'
    if 't5' in model_name:
        tokenizer = AutoTokenizer.from_pretrained("google/{}".format(model_name),
                                                  cache_dir=model_dir,
                                                  padding_side='left')
        model = AutoModelForSeq2SeqLM.from_pretrained("google/{}".format(model_name),
                                                      cache_dir=model_dir).to(device)
    elif model_name == 'gpt2':
        tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
        tokenizer.padding_side = 'left'
        model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)
    elif model_name == "falcon":
        tokenizer = AutoTokenizer.from_pretrained("tiiuae/falcon-7b-instruct".format(model_name),
                                                  cache_dir=model_dir,
                                                  trust_remote_code=True,
                                                  padding_side='left')
        model = AutoModelForCausalLM.from_pretrained("tiiuae/falcon-7b-instruct".format(model_name),
                                                     cache_dir=model_dir,
                                                     trust_remote_code=True).to(device)
    elif model_name == 'llama2':
        tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf",
                                                  cache_dir=model_dir,
                                                  trust_remote_code=True,
                                                  use_auth_token=access_token,
                                                  padding_side='left')
        model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf",
                                                     cache_dir=model_dir,
                                                     trust_remote_code=True,
                                                     use_auth_token=access_token).to(device)

    tokenizer.pad_token = tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    return tokenizer, model


def generate_text(prompts, tokenizer, model, device):
    input_strings = [re.sub(' +', ' ', prompt) for prompt in prompts]

    # Encode the input strings
    encoding = tokenizer(input_strings, return_tensors='pt', padding=True, truncation=True, max_length=1024).to(
        device)
    input_ids = encoding['input_ids']
    attention_mask = encoding['attention_mask']

    max_length = max([len(seq) for seq in input_ids]) + 50

    print("max_length:", max_length)

    outputs = model.generate(input_ids,
                             pad_token_id=tokenizer.pad_token_id,
                             do_sample=True,  # Set to True to implement top-p sampling
                             max_length=max_length,
                             # max_new_tokens=max_length,
                             top_p=0.9,  # Adjust as needed
                             temperature=0.85,
                             num_return_sequences=1)

    # # Decode only the new tokens that were generated by the model after the input tokens
    # if model_name in ['gpt2', 'llama-3b', 'llama-7b', 'falcon-7b-instruct', 'llama2']:
    #     decoded_outputs = tokenizer.batch_decode(outputs[:, len(input_ids[0]):], skip_special_tokens=True)
    # else:
    #     decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    decoded_outputs = tokenizer.batch_decode(outputs[:, len(input_ids[0]):], skip_special_tokens=True)

    return decoded_outputs


def generate_text_openai(prompts, model_name, device):
    model_dir = '/home/haolun/projects/ctb-lcharlin/haolun/saved_LLM'

    MAX_RETRIES = 3
    RETRY_DELAY = 60  # seconds

    if model_name in ['gpt3.5', 'gpt4']:
        if model_name == 'gpt3.5':
            real_model_name = 'gpt-3.5-turbo-16k'
            # real_model_name = 'text-davinci-003'
        elif model_name == 'gpt4':
            real_model_name = 'gpt-4'

        openai.api_key = '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'  # Set your OpenAI API key

        messages_contents = []
        if real_model_name in ['gpt-3.5-turbo', 'gpt-3.5-turbo-16k']:
            for prompt in prompts:
                for attempt in range(MAX_RETRIES):
                    try:
                        response = openai.ChatCompletion.create(
                            model=real_model_name,
                            messages=[
                                {
                                    "role": "system",
                                    "content": "You are a helpful assistant for movie summary."
                                },
                                {
                                    "role": "user",
                                    "content": prompt
                                }
                            ],
                            max_tokens=50  # limit the token length of response
                        )

                        message_content = response['choices'][0]['message']['content']

                        # Remove the assistant's name and the final newline from the message content
                        message_content = message_content.replace("Assistant: ", "").strip()

                        messages_contents.append(message_content)
                        break  # If successful, break out of the retry loop

                    except openai.error.APIError as e:
                        if e.error['code'] == 502 and attempt < MAX_RETRIES - 1:
                            print(f"Encountered a 502 error on prompt '{prompt}'. Retrying in {RETRY_DELAY} seconds...")
                            time.sleep(RETRY_DELAY)
                        else:
                            print(f"Failed after {attempt + 1} attempts on prompt '{prompt}'.")
                            raise

        else:
            # This is for non-chat models like text-davinci-003:
            for prompt in prompts:
                for attempt in range(MAX_RETRIES):
                    try:
                        response = openai.Completion.create(
                            model=real_model_name,
                            prompt=prompt,
                            max_tokens=50  # limit the token length of response
                        )
                        message_content = response['choices'][0]['text'].strip()
                        messages_contents.append(message_content)
                        break  # If successful, break out of the retry loop

                    except openai.error.APIError as e:
                        if e.error['code'] == 502 and attempt < MAX_RETRIES - 1:
                            print(f"Encountered a 502 error on prompt '{prompt}'. Retrying in {RETRY_DELAY} seconds...")
                            time.sleep(RETRY_DELAY)
                        else:
                            print(f"Failed after {attempt + 1} attempts on prompt '{prompt}'.")
                            raise

        return messages_contents


def save_to_file(user_summary_dict, global_path, data_name, model_name, in_context, only_title):
    save_path = f'{global_path}/saved_user_summary/{data_name}/user_summary_{model_name}_in{in_context}_title{only_title}.json'

    # Load existing data from file
    existing_data = {}
    if os.path.exists(save_path):
        with open(save_path, 'r') as f:
            content = f.read()
            if content:  # Check if the file is not empty
                existing_data = json.loads(content)

    # Merge dictionaries for the same user id
    for user_id, genres in user_summary_dict.items():
        if user_id in existing_data:
            existing_data[user_id].update(genres)
        else:
            existing_data[user_id] = genres

    # Save the merged data back to the file
    with open(save_path, 'w') as f:
        json.dump(existing_data, f)


def in_context_user_summary(built_context, model_name, device, data_name, dataloader, in_context, only_title):
    # Load model
    num_last_movies = 10
    if model_name in ['gpt3.5', 'gpt4']:
        pass
    else:
        tokenizer, model = load_tokenizer_model(model_name, device)
    # Load the training set JSON file
    with open(
            f'{global_path}/data_preprocessed/{data_name}/train_set_leave_one.json') as input_file:
        user_data_input = json.load(input_file)

    # Pre-build the demonstration examples string
    if in_context == 1:
        demo_str = ""
        for demo in built_context:
            for movie in demo['movies']:
                title = movie['Title']
                description = movie['Description'].split(".")[0]  # Use the first sentence as the short description
                demo_str += f"\n{title}: {description}"
            demo_str += f"\nSummary: {demo['User query as summary']}\n"

    user_summary_dict = {}  # Create a dictionary for storing user summaries

    batch_num = 0
    for batch in tqdm(dataloader, desc="Processing batches", unit="batch"):
        batch_start_time = time.time()
        user_id_genre_pairs = list(zip(batch["user_id"], batch["genre"]))
        # print("batch:", batch_num)
        prompts = []

        for user_id, genre in user_id_genre_pairs:
            # print(f"user_id:{user_id}, genre:{genre}")
            # Get the movies for the specified user ID and genre
            movies = user_data_input.get(str(user_id), {}).get(genre, [])[-num_last_movies:]

            # Prepare the prompt with example pairs
            if in_context == 1:
                prompt = "Task: Given a list of movies, generate a high-level summary. The following are some examples:\n"
                prompt += demo_str  # add demonstration
                prompt += f"\nNow, based on the above examples, please generate a high-level summary for the following movies of genre: {genre}\n: "
            else:
                prompt = f"Task: Given the following movies of genre {genre}, please generate a high-level summary. \n"

            for movie in movies:
                title = movie['title']
                description = movie['summary'].split(".")[0]  # Use the first sentence as the short description
                if only_title == 1:
                    prompt += f"\n{title}"
                else:
                    prompt += f"\n{title}: {description}"

            prompt += f"\nNow, please generate the summary based on the common elements of above movies. Do not mention any specific movie titles."
            prompt += "Always start with 'Summary:'. Please be concise and constrain the length within 50 tokens."

            prompts.append((prompt, user_id, genre))

        # Combine all the prompts into one list and generate the user summary
        if model_name in ['gpt3.5', 'gpt4']:
            user_summary = generate_text_openai([p[0] for p in prompts], model_name=model_name, device=device)

            user_summary_list = []

            for i, user_summary_output in enumerate(user_summary):
                user_summary_output = user_summary_output.replace('\n', ' ')
                user_summary_list.append(user_summary_output)

                user_id, genre = prompts[i][1], prompts[i][2]
                if user_id not in user_summary_dict:
                    user_summary_dict[user_id] = {}  # Initialize a new dictionary for this user ID
                user_summary_dict[user_id][genre] = user_summary_output  # Save the decoded output
        else:
            user_summary = generate_text([p[0] for p in prompts], tokenizer, model, device)
            # Loop over the decoded outputs and store them in a lists
            user_summary_list = []
            for i, decoded_output in enumerate(user_summary):
                decoded_output = decoded_output.replace('\n', ' ')
                user_summary_list.append(decoded_output)
                # Save the decoded output to a file
                user_id, genre = prompts[i][1], prompts[i][2]
                if user_id not in user_summary_dict:
                    user_summary_dict[user_id] = {}  # Initialize a new dictionary for this user ID
                user_summary_dict[user_id][genre] = decoded_output  # Save the decoded output

        # Save every 10 batches
        if batch_num % 1 == 0:
            save_to_file(user_summary_dict, global_path, data_name, model_name, in_context, only_title)
            user_summary_dict.clear()  # Clear the dictionary for the next batch of results

        batch_num += 1

    # Check for any remaining data after all batches are processed
    if user_summary_dict:
        save_to_file(user_summary_dict, global_path, data_name, model_name, in_context, only_title)


def in_context_retrieval(built_context, user_id_genre_pairs, user_summary_lists, movie_candidates_list, model_name,
                         device, data_name, batch=False):
    # Load the training set JSON file
    with open(
            f'{global_path}/data_preprocessed/ml-1m/train_set_leave_one.json') as input_file:
        user_data_input = json.load(input_file)

    prompts = []
    for (user_id, genre), user_summary_list, movie_candidates in zip(user_id_genre_pairs, user_summary_lists,
                                                                     movie_candidates_list):
        # Get the movies for the specified user ID and genre
        watched_movies = user_data_input.get(str(user_id), {}).get(genre, [])[
                         -20:]  # only use the last movie titles due to length limit

        # Prepare the prompt with example pairs
        prompt = "Task: Based on the user's preference summary, retrievel top-10 most relevant movies from the movie candidates in a ranked order, where the top movies should be more relevant. "
        prompt += "The output template should be: \"{movie title 1}\", \"{movie title 2}\", \"{movie title 3}\", ... \n"

        # Format movie candidates for the prompt
        formatted_movie_candidates = ', '.join(f'\"{movie}\"' for movie in movie_candidates)

        # Add user summary and movie candidates to the prompt
        prompt += f"\n\nUser preference summary: {user_summary_list[0]}\n"
        prompt += f"\nMovies watched by the user: \n"
        for movie in watched_movies:
            title = movie['title']
            prompt += f"\"{title}\", "

        prompt += f"\n\nNow, you should only select top-10 most relevant movie titles existing in the movie candidates for the given user summary. \nMovie candidates: {formatted_movie_candidates}\n"

        prompts.append((prompt, user_id, genre))

    # If we are not in batch mode, we generate the text for each prompt separately
    if not batch:
        # Loop over the prompts and generate the retrieved movie titles for each
        for prompt, user_id, genre in prompts:
            # Print the final prompt
            print("************************")
            print("***Prompt for user retrieval generation:***\n-----\n", prompt)
            print("************************")
            decoded_outputs = generate_text(prompt, model_name, device)
            for i, decoded_output in enumerate(decoded_outputs):
                decoded_output = decoded_output.replace('\n', ', ')
                # Save the decoded output to a file
                directory = f'{global_path}/saved_retrieved_movies/{data_name}/{user_id}'
                os.makedirs(directory, exist_ok=True)
                with open(f'{directory}/{genre}.txt', 'w') as f:
                    f.write(decoded_output)
                print(f"\nRetrieved movie titles:\n{decoded_output}")
    # If we are in batch mode, we generate the text for all prompts at once
    else:
        # Combine all the prompts into one list and generate the retrieved movie titles
        decoded_outputs = generate_text([p[0] for p in prompts], model_name, device)
        for i, decoded_output in enumerate(decoded_outputs):
            decoded_output = decoded_output.replace('\n', ', ')
            # Save the decoded output to a file
            user_id, genre = prompts[i][1], prompts[i][2]
            directory = f'{global_path}/saved_retrieved_movies/{data_name}/{user_id}'
            os.makedirs(directory, exist_ok=True)
            with open(f'{directory}/{genre}.txt', 'w') as f:
                f.write(decoded_output)
            print(f"\nRetrieved movie titles:\n{decoded_output}")
