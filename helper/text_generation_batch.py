from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from transformers import LlamaTokenizer, LlamaForCausalLM
from transformers import pipeline
import torch
import re
import openai  # you should import the module before using it

access_token = "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"


def generate_text(prompts, model_name, device):
    model_dir = '/home/haolun/projects/ctb-lcharlin/haolun/saved_LLM'
    if model_name == 'gpt3.5':

        openai.api_key = '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'  # Set your OpenAI API key

        messages_contents = []
        for prompt in prompts:
            response = openai.ChatCompletion.create(
                model='gpt3.5-turbo',
                messages=[
                    {
                        "role": "system",
                        "content": "You are a helpful assistant."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            )

            message_content = response['choices'][0]['message']['content']

            # Remove the assistant's name and the final newline from the message content
            message_content = message_content.replace("Assistant: ", "").strip()

            messages_contents.append(message_content)

        return messages_contents

    if 't5' in model_name:
        tokenizer = AutoTokenizer.from_pretrained("google/{}".format(model_name),
                                                  cache_dir=model_dir,
                                                  padding_side='left')
        model = AutoModelForSeq2SeqLM.from_pretrained("google/{}".format(model_name),
                                                      cache_dir=model_dir).to(device)
    elif model_name == 'gpt2':
        tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
        tokenizer.padding_side = 'left'
        model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)
    elif model_name == "falcon-7b-instruct":
        tokenizer = AutoTokenizer.from_pretrained("tiiuae/{}".format(model_name),
                                                  cache_dir=model_dir,
                                                  trust_remote_code=True,
                                                  padding_side='left')
        model = AutoModelForCausalLM.from_pretrained("tiiuae/{}".format(model_name),
                                                     cache_dir=model_dir,
                                                     trust_remote_code=True).to(device)
    elif model_name == 'llama2':
        tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-3b-hf",
                                                  cache_dir=model_dir,
                                                  trust_remote_code=True,
                                                  use_auth_token=access_token,
                                                  padding_side='left')
        model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-3b-hf",
                                                     cache_dir=model_dir,
                                                     trust_remote_code=True,
                                                     use_auth_token=access_token).to(device)

    tokenizer.pad_token = tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    input_strings = [re.sub(' +', ' ', prompt) for prompt in prompts]

    # Encode the input strings
    encoding = tokenizer(input_strings, return_tensors='pt', padding=True, truncation=True, max_length=1024).to(device)
    input_ids = encoding['input_ids']
    attention_mask = encoding['attention_mask']

    # # Get the length of the longest sequence
    # max_length = max(len(ids) for ids in input_ids) + 20

    # # Make sure max_length does not exceed model's maximum limit
    # if max_length > 1024:
    #     max_length = 1024

    # if model_name in ['gpt2', 'llama-3b', 'llama-7b', 'falcon-7b-instruct', 'llama2']:
    #     max_length = 300

    max_length = max([len(seq) for seq in input_ids]) + 20

    print("max_length:", max_length)

    outputs = model.generate(input_ids,
                             pad_token_id=tokenizer.pad_token_id,
                             do_sample=True,  # Set to True to implement top-p sampling
                             max_length=max_length,
                             # max_new_tokens=max_length,
                             top_p=0.85,  # Adjust as needed
                             temperature=0.8,
                             num_return_sequences=1)

    # Decode only the new tokens that were generated by the model after the input tokens
    if model_name in ['gpt2', 'llama-3b', 'llama-7b', 'falcon-7b-instruct', 'llama2']:
        decoded_outputs = tokenizer.batch_decode(outputs[:, len(input_ids[0]):], skip_special_tokens=True)
    else:
        decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)

    return decoded_outputs
